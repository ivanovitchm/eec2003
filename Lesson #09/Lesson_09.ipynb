{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson #09.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YRy_5eGBhJJi"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9M6TvG9l022"
      },
      "source": [
        "# 1.0 Network as feature extractors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhyRH1BkmRqi"
      },
      "source": [
        "Over this lesson, we’ll be discussing the concept of **transfer learning**, \n",
        "\n",
        "> the ability to use a pre-trained model as a “shortcut” to learn patterns from data it was not originally trained on.\n",
        "\n",
        "Consider a traditional machine learning scenario where we are given two classification challenges.\n",
        "\n",
        "**In the first challenge**, our goal is to train a Convolutional Neural Network to recognize dogs\n",
        "vs. cats in an image.\n",
        "\n",
        "Then, **in the second project**, we are tasked with recognizing three separate species of bears:\n",
        "grizzly bears, polar bears, and giant pandas. Using standard practices in **machine learning, neural networks, and deep learning**, we would treat these challenges as two separate problems. \n",
        "\n",
        "- First, we would gather a sufficient labeled dataset of dogs and cats, followed by training a model on the dataset. \n",
        "- We would then repeat the process a second time, only this time, gathering images of our\n",
        "bear breeds, and then training a model on top of the labeled bear dataset.\n",
        "\n",
        "\n",
        "Transfer learning proposes a different training paradigm – \n",
        "\n",
        "> what if we could use an existing pretrained classifier and use it as a starting point for a new classification task?\n",
        "\n",
        "In context of the proposed challenges above, **we would first train a Convolutional Neural Network to recognize dogs versus cats**. \n",
        "\n",
        "> Then, we would use the same CNN trained on dog and cat data to be used to\n",
        "distinguish between bear classes, even though no bear data was mixed with the dog and cat data.\n",
        "\n",
        "\n",
        "Does this sound too good to be true? It’s actually not. **Deep neural networks trained on\n",
        "large-scale datasets such as ImageNet have demonstrated to be excellent at the task of transfer\n",
        "learning**. These networks learn a set of rich, discriminating features to recognize 1,000 separate object classes. It makes sense that these filters can be reused for classification tasks other than what the CNN was originally trained on.\n",
        "\n",
        "In general, **there are two types of transfer learning** when applied to deep learning for **computer vision**:\n",
        "\n",
        "1. Treating networks as arbitrary feature extractors.\n",
        "2. Removing the fully-connected layers of an existing network, placing new FC layer set on\n",
        "top of the CNN, and fine-tuning these weights (and optionally previous layers) to recognize\n",
        "object classes.\n",
        "\n",
        "\n",
        "In this section, we’ll be focusing primarily on the first method of transfer learning, treating networks as feature extractors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLv0dSxrnwwK"
      },
      "source": [
        "## 1.1 Extracting features with a pre-trained CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAGBKpo2n-Zq"
      },
      "source": [
        "Up until this point, we have treated Convolutional Neural Networks as end-to-end image classifiers:\n",
        "\n",
        "1. We input an image to the network.\n",
        "2. The image forward propagates through the network.\n",
        "3. We obtain the final classification probabilities from the end of the network.\n",
        "\n",
        "However, **there is no “rule” that says we must allow the image to forward propagate through\n",
        "the entire network**. \n",
        "\n",
        "> Instead, we can stop the propagation at an arbitrary layer, such as an activation\n",
        "or pooling layer, extract the values from the network at this time, and then use them as feature\n",
        "vectors. \n",
        "\n",
        "For example, let’s consider the VGG16 network architecture by [Simonyan and Zisserman](https://arxiv.org/abs/1409.1556) (Figure below, left).\n",
        "\n",
        "<center><img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1CNy_EpEVeVAn7LJbPyeZm7xKW1rLnwdz\"></center><center><b>Left</b>: The original VGG16 network architecture that outputs probabilities for each of the 1,000 ImageNet class labels. <b>Right</b>: Removing the FC layers from VGG16 and instead returning the output of the final POOL layer. This output will serve as our extracted features.</center>\n",
        "\n",
        "Along with the layers in the network, we have also included the input and output shapes of the\n",
        "volumes for each layer. When treating networks as a feature extractor, we essentially “chop off” the network at an arbitrary point (normally prior to the fully-connected layers, but it really depends on your particular dataset).\n",
        "\n",
        "Now the last layer in our network is a max pooling layer (Figure above, right) which will have the output shape of 7 x 7 x 512 implying there are 512 filters each of size 7 x 7. If we were to forward propagate an image through this network with its FC head removed, we would be left with 512, 7x7 activations that have either activated or not based on the image contents. Therefore, we can actually take these 7x7x512 = 25,088 values and treat them as a feature vector that **quantifies the contents of an image**.\n",
        "\n",
        "If we repeat this process for an entire dataset of images (including datasets that VGG16 was\n",
        "not trained on), we’ll be left with a design matrix of N images, each with 25,088 columns used to\n",
        "quantify their contents (i.e., feature vectors). Given our feature vectors, we can train an off-the-shelf machine learning model such a Linear SVM, Logistic Regression classifier, or Random Forest on top of these features to obtain a classifier that recognizes new classes of images.\n",
        "\n",
        "Keep in mind that the CNN itself is not capable of recognizing these new classes – instead,\n",
        "we are using the CNN as an intermediary feature extractor. The downstream machine learning\n",
        "classifier will take care of learning the underlying patterns of the features extracted from the CNN.\n",
        "\n",
        "Later in this section, we’ll be demonstrating how you can use pre-trained CNNs (specifically\n",
        "VGG16) and the Keras library to obtain > 90% classification accuracy on image datasets such as\n",
        "Animals, CALTECH-101, and Flowers-17. Neither of these datasets contain images that VGG16\n",
        "was trained on, but by applying transfer learning, we are able to build super accurate image\n",
        "classifiers with little effort. The trick is extracting these features and storing them in an efficient manner. To accomplish this task, we’ll need HDF5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zisp6OplmCSt"
      },
      "source": [
        "## 1.2 Writing features to an HDF5 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F96t7jk8mOXZ"
      },
      "source": [
        "Before we can even think about treating CNN Architectures as a feature extractor, we\n",
        "first need to develop a bit of infrastructure. In particular, we need to define a Python class\n",
        "named HDF5DatasetWriter, which as the name suggests, is responsible for taking an input set of\n",
        "NumPy arrays (whether features, raw images, etc.) and writing them to HDF5 format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrT62Vkhv2NZ"
      },
      "source": [
        "# import the necessary packages\n",
        "import h5py\n",
        "import os\n",
        "\n",
        "class HDF5DatasetWriter:\n",
        "  def __init__(self, dims, outputPath, dataKey=\"images\",bufSize=1000):\n",
        "    \"\"\"\n",
        "    The constructor to HDF5DatasetWriter accepts four parameters, two of which are optional.\n",
        "    \n",
        "    Args:\n",
        "    dims: controls the dimension or shape of the data we will be storing in the dataset.\n",
        "    if we were storing the (flattened) raw pixel intensities of the 28x28 = 784 MNIST dataset, \n",
        "    then dims=(70000, 784).\n",
        "    outputPath: path to where our output HDF5 file will be stored on disk.\n",
        "    datakey: The optional dataKey is the name of the dataset that will store\n",
        "    the data our algorithm will learn from.\n",
        "    bufSize: controls the size of our in-memory buffer, which we default to 1,000 feature\n",
        "    vectors/images. Once we reach bufSize, we’ll flush the buffer to the HDF5 dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # check to see if the output path exists, and if so, raise\n",
        "    # an exception\n",
        "    if os.path.exists(outputPath):\n",
        "      raise ValueError(\"The supplied `outputPath` already \"\n",
        "        \"exists and cannot be overwritten. Manually delete \"\n",
        "        \"the file before continuing.\", outputPath)\n",
        "\n",
        "    # open the HDF5 database for writing and create two datasets:\n",
        "    # one to store the images/features and another to store the\n",
        "    # class labels\n",
        "    self.db = h5py.File(outputPath, \"w\")\n",
        "    # \n",
        "    # for resource limitations due to hard-disk space, a compression algorithm can be used, the price is the demand of computational power\n",
        "    #\n",
        "    self.data = self.db.create_dataset(dataKey, dims,dtype=\"float\")#compression='gzip')\n",
        "    self.labels = self.db.create_dataset(\"labels\", (dims[0],),dtype=\"int\")\n",
        "\n",
        "    # store the buffer size, then initialize the buffer itself\n",
        "    # along with the index into the datasets\n",
        "    self.bufSize = bufSize\n",
        "    self.buffer = {\"data\": [], \"labels\": []}\n",
        "    self.idx = 0\n",
        "\n",
        "  def add(self, rows, labels):\n",
        "    # add the rows and labels to the buffer\n",
        "    self.buffer[\"data\"].extend(rows)\n",
        "    self.buffer[\"labels\"].extend(labels)\n",
        "\n",
        "    # check to see if the buffer needs to be flushed to disk\n",
        "    if len(self.buffer[\"data\"]) >= self.bufSize:\n",
        "      self.flush()\n",
        "\n",
        "  def flush(self):\n",
        "    # write the buffers to disk then reset the buffer\n",
        "    i = self.idx + len(self.buffer[\"data\"])\n",
        "    self.data[self.idx:i] = self.buffer[\"data\"]\n",
        "    self.labels[self.idx:i] = self.buffer[\"labels\"]\n",
        "    self.idx = i\n",
        "    self.buffer = {\"data\": [], \"labels\": []}\n",
        "\n",
        "  def storeClassLabels(self, classLabels):\n",
        "    # create a dataset to store the actual class label names,\n",
        "    # then store the class labels\n",
        "    dt = h5py.special_dtype(vlen=str) # `vlen=unicode` for Py2.7\n",
        "    labelSet = self.db.create_dataset(\"label_names\",(len(classLabels),), dtype=dt)\n",
        "    labelSet[:] = classLabels\n",
        "\n",
        "  def close(self):\n",
        "    # check to see if there are any other entries in the buffer\n",
        "    # that need to be flushed to disk\n",
        "    if len(self.buffer[\"data\"]) > 0:\n",
        "      self.flush()\n",
        "\n",
        "    # close the dataset\n",
        "    self.db.close()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3uegkGTcPdB"
      },
      "source": [
        "As you can see, the **HDF5DatasetWriter** doesn’t have much to do with machine learning or\n",
        "deep learning at all – it’s simply a class used to help us store data in HDF5 format. As you continue\n",
        "in your deep learning career, you’ll notice that much of the initial labor when setting up a new\n",
        "problem is getting the data into a format you can work with. Once you have your data in a format\n",
        "that’s straightforward to manipulate, it becomes substantially easier to apply machine learning and\n",
        "deep learning techniques to your data.\n",
        "\n",
        "Now that our **HDF5DatasetWriter** is implemented, we can move on to actually extracting\n",
        "features using pre-trained Convolutional Neural Networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wgghb6qvqaXD"
      },
      "source": [
        "## 1.3 The feature extraction process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MiEPB9jwSZG"
      },
      "source": [
        "Let’s define a Python script that can be used to extract features from an arbitrary image dataset (provided the input dataset follows a specific directory structure)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MagMf-0LkCin"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import progressbar\n",
        "import h5py\n",
        "import random\n",
        "import os"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc1mxvEANFRU",
        "outputId": "43d9559c-0496-408d-b41a-69e0e12c2ee8"
      },
      "source": [
        "!pip install gdown"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCUM0shSMYWF",
        "outputId": "852825e6-1e38-450c-e799-ff30347fe6d1"
      },
      "source": [
        "# download animals dataset\n",
        "!gdown https://drive.google.com/uc?id=1ZkrEbDEdiSjpog6IcWK-HB2Y3uk2WjFE\n",
        "\n",
        "# download caltech-101 dataset\n",
        "!gdown https://drive.google.com/uc?id=1VpcNjEFHbtfZbQx7Q9FvRBlCYFwxYPIS\n",
        "\n",
        "# download flowers dataset\n",
        "!gdown https://drive.google.com/uc?id=1o_BeSmvyuelAyEYpGPNphlkX4bfQy2r5"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZkrEbDEdiSjpog6IcWK-HB2Y3uk2WjFE\n",
            "To: /content/animals.zip\n",
            "197MB [00:01, 122MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VpcNjEFHbtfZbQx7Q9FvRBlCYFwxYPIS\n",
            "To: /content/caltech-101.zip\n",
            "121MB [00:00, 148MB/s]  \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1o_BeSmvyuelAyEYpGPNphlkX4bfQy2r5\n",
            "To: /content/flowers17.zip\n",
            "60.5MB [00:00, 118MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpYqrMqMNSSq"
      },
      "source": [
        "!unzip animals.zip\n",
        "!unzip caltech-101.zip\n",
        "!unzip flowers17.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1Jhl_nN0MzE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f45561-d9f9-4786-8732-91699e65f11b"
      },
      "source": [
        "# \n",
        "# Last layer before the head has a 7x7x512 dimension\n",
        "#\n",
        "model = VGG16(weights=\"imagenet\", include_top=True)\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 7s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36ny6w-yq4Ls"
      },
      "source": [
        "def feature_extraction(dataset,output,buffer_size,bs):\n",
        "\t\t'''\n",
        "\t\t\tdataset: input folder with images dataset\n",
        "\t\t\toutput: folder to store the feature extraction\n",
        "\t\t\tbuffer_size: controls the size of our in-memory buffer\n",
        "\t\t\tbs: batch size\n",
        "\t\t'''\n",
        "\n",
        "\t\t# grab the list of images that we'll be describing then randomly\n",
        "\t\t# shuffle them to allow for easy training and testing splits via\n",
        "\t\t# array slicing during training time\n",
        "\t\tprint(\"[INFO] loading images...\")\n",
        "\t\timagePaths = list(paths.list_images(dataset))\n",
        "\t\trandom.shuffle(imagePaths)\n",
        "\n",
        "\t\t# extract the class labels from the image paths then encode the\n",
        "\t\t# labels\n",
        "\t\tlabels = [p.split(os.path.sep)[-2] for p in imagePaths]\n",
        "\t\tle = LabelEncoder()\n",
        "\t\tlabels = le.fit_transform(labels)\n",
        "\n",
        "\t\t# load the VGG16 network\n",
        "\t\tprint(\"[INFO] loading network...\")\n",
        "\t\tmodel = VGG16(weights=\"imagenet\", include_top=False)\n",
        "\n",
        "\t\t# initialize the HDF5 dataset writer, then store the class label\n",
        "\t\t# names in the dataset\n",
        "\t\tdataset = HDF5DatasetWriter((len(imagePaths), 512 * 7 * 7),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\toutput, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdataKey=\"features\", \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tbufSize=buffer_size)\n",
        "\t\tdataset.storeClassLabels(le.classes_)\n",
        "\n",
        "\t\t# initialize the progress bar\n",
        "\t\twidgets = [\"Extracting Features: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
        "\t\tpbar = progressbar.ProgressBar(maxval=len(imagePaths),widgets=widgets).start()\n",
        "\n",
        "\t\t# loop over the images in batches\n",
        "\t\tfor i in np.arange(0, len(imagePaths), bs):\n",
        "\t\t\t# extract the batch of images and labels, then initialize the\n",
        "\t\t\t# list of actual images that will be passed through the network\n",
        "\t\t\t# for feature extraction\n",
        "\t\t\tbatchPaths = imagePaths[i:i + bs]\n",
        "\t\t\tbatchLabels = labels[i:i + bs]\n",
        "\t\t\tbatchImages = []\n",
        "\n",
        "\t\t\t# loop over the images and labels in the current batch\n",
        "\t\t\tfor (j, imagePath) in enumerate(batchPaths):\n",
        "\t\t\t\t# load the input image using the Keras helper utility\n",
        "\t\t\t\t# while ensuring the image is resized to 224x224 pixels\n",
        "\t\t\t\timage = load_img(imagePath, target_size=(224, 224))\n",
        "\t\t\t\timage = img_to_array(image)\n",
        "\n",
        "\t\t\t\t# preprocess the image by (1) expanding the dimensions and\n",
        "\t\t\t\t# (2) subtracting the mean RGB pixel intensity from the\n",
        "\t\t\t\t# ImageNet dataset\n",
        "\t\t\t\timage = np.expand_dims(image, axis=0)\n",
        "\t\t\t\timage = imagenet_utils.preprocess_input(image)\n",
        "\n",
        "\t\t\t\t# add the image to the batch\n",
        "\t\t\t\tbatchImages.append(image)\n",
        "\n",
        "\t\t\t# pass the images through the network and use the outputs as\n",
        "\t\t\t# our actual features\n",
        "\t\t\tbatchImages = np.vstack(batchImages)\n",
        "\t\t\tfeatures = model.predict(batchImages, batch_size=bs)\n",
        "\n",
        "\t\t\t# reshape the features so that each image is represented by\n",
        "\t\t\t# a flattened feature vector of the `MaxPooling2D` outputs\n",
        "\t\t\tfeatures = features.reshape((features.shape[0], 512 * 7 * 7))\n",
        "\n",
        "\t\t\t# add the features and labels to our HDF5 dataset\n",
        "\t\t\tdataset.add(features, batchLabels)\n",
        "\t\t\tpbar.update(i)\n",
        "\n",
        "\t\t# close the dataset\n",
        "\t\tdataset.close()\n",
        "\t\tpbar.finish()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE6N03sCR9uP"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import pickle\n",
        "import h5py\n",
        "\n",
        "def train_and_evaluate(features_set):\n",
        "    db = h5py.File(features_set,mode='r')\n",
        "    print(\"Database keys {0:}\".format(list(db.keys())))\n",
        "\n",
        "    # open the HDF5 database for reading then determine the index of\n",
        "    # the training and testing split, provided that this data was\n",
        "    # already shuffled *prior* to writing it to disk\n",
        "    i = int(db[\"labels\"].shape[0] * 0.75)\n",
        "\n",
        "    # define the set of parameters that we want to tune then start a\n",
        "    # grid search where we evaluate our model for each value of C\n",
        "    print(\"[INFO] tuning hyperparameters...\")\n",
        "    params = {\"C\": [0.1, 1.0, 10.0]}\n",
        "    model = GridSearchCV(LogisticRegression(solver=\"lbfgs\",\n",
        "                                            multi_class=\"auto\"),\n",
        "                        params, \n",
        "                        cv=3, \n",
        "                        n_jobs=-1)\n",
        "\n",
        "    model.fit(db[\"features\"][:i], db[\"labels\"][:i])\n",
        "    print(\"[INFO] best hyperparameters: {}\".format(model.best_params_))\n",
        "\n",
        "    # evaluate the model\n",
        "    print(\"[INFO] evaluating...\")\n",
        "    preds = model.predict(db[\"features\"][i:])\n",
        "\n",
        "    print(classification_report(db[\"labels\"][i:], \n",
        "                                preds,\n",
        "                                target_names=[str(i,'utf-8') for i in db[\"label_names\"]])\n",
        "    )\n",
        "    # serialize the model to disk\n",
        "    print(\"[INFO] saving model...\")\n",
        "    f = open(features_set.split(\"/\")[0] + \".cpickle\", \"wb\")\n",
        "    f.write(pickle.dumps(model.best_estimator_))\n",
        "    f.close()\n",
        "\n",
        "    # close the database\n",
        "    db.close()"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kana_F16RG5x"
      },
      "source": [
        "### 1.3.1 Animals dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veqc-zoW3WZk"
      },
      "source": [
        "\n",
        "The first dataset we are going to extract features from using VGG16 is our “Animals” dataset. This dataset consists of 3,000 images, of three classes: dogs, cats, and pandas. Notice how the .shape is (3000, 25088) – this result implies that each of the 3,000 images in our Animals dataset is quantified via feature vector with length 25,088 (i.e., the values inside **VGG16** after the final POOL operation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIgpmBPaPnS7"
      },
      "source": [
        "# INPUTS\n",
        "# path to input dataset\n",
        "dataset = \"animals\"\n",
        "\n",
        "# path to output HDF5 file\n",
        "output  = \"animals/hdf5/features.hdf5\"\n",
        "\n",
        "# size of feature extraction buffer\n",
        "buffer_size = 1000\n",
        "\n",
        "# store the batch size in a convenience variable\n",
        "bs = 32"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOzS-WGuQSB4"
      },
      "source": [
        "feature_extraction(dataset,output,buffer_size,bs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lurxOifvu1ul",
        "outputId": "fe5c8349-01f3-49a3-e3c8-dc2378da12a7"
      },
      "source": [
        "db = h5py.File(output,mode='r')\n",
        "list(db.keys())"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['features', 'label_names', 'labels']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TMfKV702WwL",
        "outputId": "686377e4-ac52-4451-82f2-f6eb3781a1eb"
      },
      "source": [
        "db[\"features\"].shape"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 25088)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD0Jv4qm3dOr",
        "outputId": "09499638-2ad2-4767-9389-51e79affbc08"
      },
      "source": [
        "db[\"labels\"].shape"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYNURAtd3gxH",
        "outputId": "d7347e9d-ff63-47d6-8131-b9a7d2894346"
      },
      "source": [
        "db[\"label_names\"].shape"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJEbKk6sAEte",
        "outputId": "7792d68f-a455-46d9-8be2-d9fc17ccd49b"
      },
      "source": [
        "[str(i,'utf-8') for i in db[\"label_names\"]]"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cats', 'dogs', 'panda']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FkQSXiRTJYs",
        "outputId": "c29e29d0-abcd-472f-c711-1d6a92184a9c"
      },
      "source": [
        "train_and_evaluate(output)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Database keys ['features', 'label_names', 'labels']\n",
            "[INFO] tuning hyperparameters...\n",
            "[INFO] best hyperparameters: {'C': 0.1}\n",
            "[INFO] evaluating...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        cats       0.97      1.00      0.98       264\n",
            "        dogs       0.99      0.96      0.98       250\n",
            "       panda       1.00      1.00      1.00       236\n",
            "\n",
            "    accuracy                           0.99       750\n",
            "   macro avg       0.99      0.99      0.99       750\n",
            "weighted avg       0.99      0.99      0.99       750\n",
            "\n",
            "[INFO] saving model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_1OqSFhUtMe"
      },
      "source": [
        "### 1.3.2 Caltech-101 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RHqW_CaUtMe"
      },
      "source": [
        "Just as we extracted features from the Animals dataset, we can do the same with CALTECH-101."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyQKIQmtUtMe"
      },
      "source": [
        "# INPUTS\n",
        "# path to input dataset\n",
        "dataset = \"caltech-101\"\n",
        "\n",
        "# path to output HDF5 file\n",
        "output  = \"caltech-101/hdf5/features.hdf5\"\n",
        "\n",
        "# size of feature extraction buffer\n",
        "buffer_size = 1000\n",
        "\n",
        "# store the batch size in a convenience variable\n",
        "bs = 32"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKr-Wwj0UtMf",
        "outputId": "1ffbd77b-0e28-437b-99a9-0a35af564844"
      },
      "source": [
        "feature_extraction(dataset,output,buffer_size,bs)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n",
            "[INFO] loading network...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting Features: 100% |####################################| Time:  0:00:38\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeEuL-TuUtMf",
        "outputId": "66e1efba-1b80-4915-b290-f70384c89cd7"
      },
      "source": [
        "db = h5py.File(output,mode='r')\n",
        "list(db.keys())"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['features', 'label_names', 'labels']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqKuEarwUtMf",
        "outputId": "9422a1b4-3eae-4f3d-aa76-fa288774ef48"
      },
      "source": [
        "db[\"features\"].shape"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8677, 25088)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4eakXQtUtMg",
        "outputId": "745ee861-8ddb-428b-a3b1-77cffb0954c5"
      },
      "source": [
        "db[\"labels\"].shape"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8677,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giLEoXUCUtMg",
        "outputId": "e9704ee8-e7ec-442f-d1d3-fe9a5864d9b1"
      },
      "source": [
        "db[\"label_names\"].shape"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(101,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZtFX3x5UtMg",
        "outputId": "85d9146f-ce8a-40cb-e4db-5e50d5af94a1"
      },
      "source": [
        "# 15min - 30min or more\n",
        "train_and_evaluate(output)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Database keys ['features', 'label_names', 'labels']\n",
            "[INFO] tuning hyperparameters...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] best hyperparameters: {'C': 1.0}\n",
            "[INFO] evaluating...\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "          Faces       0.89      0.93      0.91        95\n",
            "     Faces_easy       0.94      0.96      0.95       108\n",
            "       Leopards       0.94      1.00      0.97        44\n",
            "     Motorbikes       1.00      1.00      1.00       210\n",
            "      accordion       1.00      1.00      1.00        16\n",
            "      airplanes       1.00      1.00      1.00       200\n",
            "         anchor       0.75      0.80      0.77        15\n",
            "            ant       0.86      0.80      0.83        15\n",
            "         barrel       1.00      0.91      0.95        11\n",
            "           bass       0.92      0.86      0.89        14\n",
            "         beaver       1.00      0.87      0.93        15\n",
            "      binocular       1.00      1.00      1.00         7\n",
            "         bonsai       1.00      0.97      0.98        33\n",
            "          brain       0.88      0.94      0.91        16\n",
            "   brontosaurus       0.88      0.82      0.85        17\n",
            "         buddha       0.90      1.00      0.95        18\n",
            "      butterfly       0.94      0.89      0.91        18\n",
            "         camera       1.00      0.93      0.97        15\n",
            "         cannon       1.00      0.75      0.86         8\n",
            "       car_side       1.00      1.00      1.00        30\n",
            "    ceiling_fan       0.91      1.00      0.95        10\n",
            "      cellphone       1.00      1.00      1.00        15\n",
            "          chair       0.83      0.83      0.83        18\n",
            "     chandelier       1.00      0.97      0.98        33\n",
            "    cougar_body       0.86      0.86      0.86         7\n",
            "    cougar_face       1.00      0.95      0.97        19\n",
            "           crab       1.00      0.76      0.86        25\n",
            "       crayfish       0.84      0.84      0.84        19\n",
            "      crocodile       0.86      1.00      0.92        12\n",
            " crocodile_head       0.93      0.87      0.90        15\n",
            "            cup       0.92      0.92      0.92        13\n",
            "      dalmatian       1.00      1.00      1.00         8\n",
            "    dollar_bill       1.00      0.92      0.96        12\n",
            "        dolphin       0.80      0.92      0.86        13\n",
            "      dragonfly       0.95      0.95      0.95        20\n",
            "electric_guitar       0.88      1.00      0.94        15\n",
            "       elephant       0.95      0.90      0.92        20\n",
            "            emu       1.00      1.00      1.00        10\n",
            "      euphonium       1.00      0.95      0.98        21\n",
            "           ewer       1.00      0.96      0.98        23\n",
            "          ferry       0.95      1.00      0.97        18\n",
            "       flamingo       0.94      0.94      0.94        18\n",
            "  flamingo_head       0.83      1.00      0.91         5\n",
            "       garfield       1.00      0.75      0.86        12\n",
            "        gerenuk       1.00      0.70      0.82        10\n",
            "     gramophone       1.00      0.94      0.97        16\n",
            "    grand_piano       1.00      1.00      1.00        18\n",
            "      hawksbill       0.94      1.00      0.97        17\n",
            "      headphone       0.90      0.90      0.90        10\n",
            "       hedgehog       0.82      1.00      0.90         9\n",
            "     helicopter       0.91      1.00      0.95        21\n",
            "           ibis       0.95      0.90      0.92        20\n",
            "   inline_skate       1.00      1.00      1.00         7\n",
            "    joshua_tree       0.84      1.00      0.91        16\n",
            "       kangaroo       0.87      0.93      0.90        14\n",
            "          ketch       0.81      0.86      0.83        29\n",
            "           lamp       0.82      1.00      0.90        14\n",
            "         laptop       1.00      1.00      1.00        32\n",
            "          llama       0.95      0.95      0.95        21\n",
            "        lobster       0.64      0.70      0.67        10\n",
            "          lotus       0.88      0.83      0.86        18\n",
            "       mandolin       1.00      0.90      0.95        10\n",
            "         mayfly       0.67      0.80      0.73         5\n",
            "        menorah       1.00      1.00      1.00        22\n",
            "      metronome       1.00      1.00      1.00         8\n",
            "        minaret       0.88      1.00      0.94        23\n",
            "       nautilus       0.94      1.00      0.97        15\n",
            "        octopus       0.75      0.86      0.80         7\n",
            "          okapi       1.00      0.91      0.95        11\n",
            "         pagoda       0.91      1.00      0.95        10\n",
            "          panda       1.00      0.92      0.96        12\n",
            "         pigeon       1.00      1.00      1.00         7\n",
            "          pizza       0.92      0.92      0.92        13\n",
            "       platypus       0.86      0.75      0.80         8\n",
            "        pyramid       1.00      0.93      0.97        15\n",
            "       revolver       1.00      1.00      1.00        21\n",
            "          rhino       0.93      0.93      0.93        14\n",
            "        rooster       1.00      0.93      0.96        14\n",
            "      saxophone       1.00      0.93      0.96        14\n",
            "       schooner       0.82      0.70      0.76        20\n",
            "       scissors       1.00      1.00      1.00        10\n",
            "       scorpion       0.96      1.00      0.98        24\n",
            "      sea_horse       0.88      0.93      0.90        15\n",
            "         snoopy       0.62      0.83      0.71         6\n",
            "    soccer_ball       1.00      1.00      1.00        13\n",
            "        stapler       0.93      1.00      0.97        14\n",
            "       starfish       0.96      0.89      0.92        27\n",
            "    stegosaurus       1.00      0.88      0.93        16\n",
            "      stop_sign       0.94      1.00      0.97        17\n",
            "     strawberry       1.00      0.80      0.89         5\n",
            "      sunflower       0.95      1.00      0.97        19\n",
            "           tick       1.00      1.00      1.00        17\n",
            "      trilobite       1.00      1.00      1.00        17\n",
            "       umbrella       0.95      0.90      0.93        21\n",
            "          watch       1.00      0.97      0.98        65\n",
            "    water_lilly       0.71      0.71      0.71         7\n",
            "     wheelchair       1.00      1.00      1.00        20\n",
            "       wild_cat       0.60      0.75      0.67         4\n",
            "  windsor_chair       1.00      0.93      0.97        15\n",
            "         wrench       0.86      0.86      0.86         7\n",
            "       yin_yang       1.00      0.93      0.96        14\n",
            "\n",
            "       accuracy                           0.95      2170\n",
            "      macro avg       0.93      0.92      0.92      2170\n",
            "   weighted avg       0.95      0.95      0.95      2170\n",
            "\n",
            "[INFO] saving model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrSOmdsYhJJf"
      },
      "source": [
        "### 1.3.3 Flowers-17 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvSJT0zNhJJf"
      },
      "source": [
        "Just as we extracted features from the Animals dataset, we can do the same with CALTECH-101."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Csu5b3HXhJJf"
      },
      "source": [
        "# INPUTS\n",
        "# path to input dataset\n",
        "dataset = \"flowers17\"\n",
        "\n",
        "# path to output HDF5 file\n",
        "output  = \"flowers17/hdf5/features.hdf5\"\n",
        "\n",
        "# size of feature extraction buffer\n",
        "buffer_size = 1000\n",
        "\n",
        "# store the batch size in a convenience variable\n",
        "bs = 32"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsT5dL5nhJJg",
        "outputId": "087a6ea2-d29e-4741-d0f4-0c4c1f71eb2c"
      },
      "source": [
        "feature_extraction(dataset,output,buffer_size,bs)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n",
            "[INFO] loading network...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting Features: 100% |####################################| Time:  0:00:11\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fid7qxUhJJh",
        "outputId": "22e021b7-25e6-4eda-badc-28d2e53956ed"
      },
      "source": [
        "db = h5py.File(output,mode='r')\n",
        "list(db.keys())"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['features', 'label_names', 'labels']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVYAf-cUhJJi",
        "outputId": "31f4c8a2-560e-495b-dbff-217c9cc06000"
      },
      "source": [
        "db[\"features\"].shape"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1360, 25088)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwpPo9WUhJJi",
        "outputId": "96f44922-2183-4358-88dd-51c1c5b2d90d"
      },
      "source": [
        "db[\"labels\"].shape"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1360,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfeDcLIzhJJi",
        "outputId": "9fad7203-12c0-4841-c748-68e7860bba16"
      },
      "source": [
        "db[\"label_names\"].shape"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dT7THsbhJJi",
        "outputId": "75c279dd-0813-4def-ba33-359387ce2670"
      },
      "source": [
        "# 2min\n",
        "train_and_evaluate(output)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Database keys ['features', 'label_names', 'labels']\n",
            "[INFO] tuning hyperparameters...\n",
            "[INFO] best hyperparameters: {'C': 0.1}\n",
            "[INFO] evaluating...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    bluebell       0.95      1.00      0.97        19\n",
            "   buttercup       1.00      0.87      0.93        23\n",
            "   coltsfoot       1.00      0.96      0.98        23\n",
            "     cowslip       0.67      0.80      0.73        25\n",
            "      crocus       1.00      0.90      0.95        20\n",
            "    daffodil       0.78      0.95      0.86        19\n",
            "       daisy       0.94      1.00      0.97        15\n",
            "   dandelion       1.00      0.89      0.94        19\n",
            "  fritillary       0.91      1.00      0.95        20\n",
            "        iris       1.00      0.86      0.92        21\n",
            "  lilyvalley       0.86      0.95      0.90        20\n",
            "       pansy       0.88      1.00      0.93        14\n",
            "    snowdrop       0.90      0.95      0.93        20\n",
            "   sunflower       1.00      1.00      1.00        18\n",
            "   tigerlily       1.00      0.94      0.97        18\n",
            "       tulip       0.89      0.65      0.76        26\n",
            "  windflower       0.95      1.00      0.98        20\n",
            "\n",
            "    accuracy                           0.91       340\n",
            "   macro avg       0.93      0.92      0.92       340\n",
            "weighted avg       0.92      0.91      0.91       340\n",
            "\n",
            "[INFO] saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRy_5eGBhJJi"
      },
      "source": [
        "# 2.0 Fine-tuning networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdVh5JwSiZrj"
      },
      "source": [
        "In the previous section we learned how to treat a pre-trained **Convolutional Neural Network** as **feature extractor**. \n",
        "\n",
        "> Using this feature extractor, we forward propagated our dataset of images through the network, extracted the activations at a given layer, and saved the values to disk. A standard machine\n",
        "learning classifier (in this case, Logistic Regression) was then trained on top of the CNN features.\n",
        "\n",
        "This CNN feature extractor approach, called **transfer learning**, obtained remarkable accuracy, far higher than any of our previous experiments on the Animals, CALTECH-101, or Flowers-17 dataset.\n",
        "\n",
        "But there is another type of transfer learning, one that can actually outperform the feature extraction method if you have sufficient data. This method is called **fine-tuning** and **requires us to perform “network surgery”**. \n",
        "\n",
        "1. First, we take a **scalpel and cut off the final set of fully-connected layers** (i.e., the “head” of the network) from a pre-trained /Convolutional Neural Network, such as\n",
        "VGG, ResNet, or Inception. \n",
        "2. We then **replace the head** with a new set of fully-connected layers with random initializations. From there all layers below the head are frozen so their weights cannot be\n",
        "updated (i.e., the backward pass in backpropagation does not reach them)\n",
        "3.  We then train the network **using a very small learning rate** so the new set of FC layers can start to learn patterns from the previously learned CONV layers earlier in the network. \n",
        "4. Optionally, we may unfreeze the rest of the network and continue training. Applying fine-tuning allows us to apply pre-trained networks to recognize classes that they were not originally trained on; furthermore, **this method can lead to higher accuracy than feature extraction**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwpP_T-fknnS"
      },
      "source": [
        "## 2.1 Transfer Learning and Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deGXnCSBksDi"
      },
      "source": [
        "**Fine-tuning is a type of transfer learning**. We apply fine-tuning to deep learning models that have already been trained on a given dataset. Typically, these networks are state-of-the-art architectures\n",
        "such as VGG, ResNet, and Inception that have been trained on the ImageNet dataset.\n",
        "\n",
        "As we found out in previous section on feature extraction, these networks contain rich, discriminative filters that can be used on datasets and class labels outside the ones they have already been trained on. However, instead of simply applying feature extraction, we are going to perform network surgery and modify the actual architecture so we can re-train parts of the network.\n",
        "\n",
        "\n",
        "If this sounds like something out of a bad horror movie; don’t worry, there won’t be any blood and gore – but we will have some fun and learn a lot with our experiments. To understand how finetuning\n",
        "works, consider Figure below (left) where we have the layers of the VGG16 network. As we know, the final set of layers (i.e., the “head”) are our fully-connected layers along with our softmax classifier. When performing fine-tuning, we actually remove the head from the network, just as in feature extraction (middle). However, unlike feature extraction, when we perform fine-tuning we actually **build a new fully-connected head and place it on top of the original architecture\n",
        "(right)**.\n",
        "\n",
        "\n",
        "<center><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1qTj4KeosAyDUcffqTQ_BepiINEUXs-cE\"></center><center><b>Left</b>:  The original VGG16 network architecture. <b>Middle</b>: Removing the FC layers from VGG16 and treating the final POOL layer as a feature extractor. <b>Right</b>: Removing the original FC layers and replacing them with a brand new FC head. These new FC layers can then be fine-tuned to the specific dataset (the old FC layers are no longer used).</center>\n",
        "\n",
        "\n",
        "In most cases your new FC head will have fewer parameters than the original one; however, that really depends on your particular dataset. The new FC head is randomly initialized (just like any other layer in a new network) and connected to the body of the original network, and we are ready to train.\n",
        "\n",
        "However, there is a problem – our CONV layers have already learned rich, discriminating filters while our FC layers are brand new and totally random. If we allow the gradient to backpropagate from these random values all the way through the body of our network, we risk destroying these powerful features. To circumvent this, we instead let our FC head “warm up” by (ironically) “freezing” all layers in the body of the network (I told you the cadaver analogy works well here) as\n",
        "in Figure below (left).\n",
        "\n",
        "\n",
        "\n",
        "<center><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=11Zh6mGG3qMISsnCg6JLgL-sH7TnxpUSC\"></center><center><b>Left</b>: When we start the fine-tuning process we freeze all CONV layers in the network and only allow the gradient to backpropagate through the FC layers. Doing this allows our network to “warm up”. <b>Right</b>: After the FC layers have had a chance to warm up we may choose to unfreeze all layers in the network and allow each of them to be fine-tuned as well.</center>\n",
        "\n",
        "\n",
        "Training data is forward propagated through the network as we normally would; however, the backpropagation is stopped after the FC layers, which allows these layers to start to learn patterns from the highly discriminative CONV layers. In some cases, we may never unfreeze the body of the network as our new FC head may obtain sufficient accuracy. \n",
        "\n",
        "However, for some datasets it is often advantageous to allow the original CONV layers to be modified during the fine-tuning process as\n",
        "well (Figure above, right).\n",
        "\n",
        "After the FC head has started to learn patterns in our dataset, pause training, unfreeze the body, and then continue the training, but with a very **small learning rate** – we do not want to deviate our\n",
        "CONV filters dramatically. \n",
        "\n",
        "Training is then allowed to continue until sufficient accuracy is obtained. Fine-tuning is a super powerful method to obtain image classifiers from pre-trained CNNs on custom datasets, even more powerful than feature extraction in most cases. **The downside is that\n",
        "fine-tuning can require a bit more work and your choice in FC head parameters does play a big part\n",
        "in network accuracy** – you can’t rely strictly on regularization techniques here as your network has already been pre-trained and you can’t deviate from the regularization already being performed by\n",
        "the network.\n",
        "\n",
        "Secondly, for small datasets, it can be challenging to get your network to start “learning” from a “cold” FC start, which is why we freeze the body of the network first. Even still, getting past the warm-up stage can be a bit of a challenge and might require you to use optimizers other than SGD. **While fine-tuning does require a bit more effort, if it is done correctly, you’ll nearly always enjoy higher accuracy**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaQ6aKJgaalh"
      },
      "source": [
        "## 2.2 Indexes and Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p37ZtS0IwXBs"
      },
      "source": [
        "Prior to performing **network surgery**, we need to know the **layer name and index** of every layer in a given deep learning model. We need this information as we’ll be required to **“freeze”** and **“unfreeze”** certain layers in a pre-trained CNN.\n",
        "\n",
        "Without knowing the layer names and indexes ahead of time, we would be “cutting blindly”, an out-of-control surgeon with no game plan. **If we instead take a few minutes to examine the network architecture and implementation, we can better prepare for our surgery.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ-1lFYebUiP",
        "outputId": "1b7cd76b-a643-4597-dc9e-ec43628c38ec"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "# whether or not to include top of CNN\n",
        "include_top = 0\n",
        "\n",
        "# load the VGG16 network\n",
        "print(\"[INFO] loading network...\")\n",
        "model = VGG16(weights=\"imagenet\", include_top= include_top > 0)\n",
        "print(\"[INFO] showing layers...\")\n",
        "\n",
        "# loop over the layers in the network and display them to the\n",
        "# console\n",
        "for (i, layer) in enumerate(model.layers):\n",
        "\tprint(\"[INFO] {}\\t{}\".format(i, layer.__class__.__name__))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading network...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "[INFO] showing layers...\n",
            "[INFO] 0\tInputLayer\n",
            "[INFO] 1\tConv2D\n",
            "[INFO] 2\tConv2D\n",
            "[INFO] 3\tMaxPooling2D\n",
            "[INFO] 4\tConv2D\n",
            "[INFO] 5\tConv2D\n",
            "[INFO] 6\tMaxPooling2D\n",
            "[INFO] 7\tConv2D\n",
            "[INFO] 8\tConv2D\n",
            "[INFO] 9\tConv2D\n",
            "[INFO] 10\tMaxPooling2D\n",
            "[INFO] 11\tConv2D\n",
            "[INFO] 12\tConv2D\n",
            "[INFO] 13\tConv2D\n",
            "[INFO] 14\tMaxPooling2D\n",
            "[INFO] 15\tConv2D\n",
            "[INFO] 16\tConv2D\n",
            "[INFO] 17\tConv2D\n",
            "[INFO] 18\tMaxPooling2D\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e64KpW4KzcBL"
      },
      "source": [
        "Before we can replace the head of a pre-trained CNN, we need something to replace it with – therefore, we need to define our own fully-connected head of the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZw23cpCeZ_m"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# a fully connect network\n",
        "class FCHeadNet:\n",
        "\t@staticmethod\n",
        "\tdef build(baseModel, classes, D):\n",
        "\t\t# initialize the head model that will be placed on top of\n",
        "\t\t# the base, then add a FC layer\n",
        "\t\theadModel = baseModel.output\n",
        "\t\theadModel = Flatten(name=\"flatten\")(headModel)\n",
        "\t\theadModel = Dense(D, activation=\"relu\")(headModel)\n",
        "\t\theadModel = Dropout(0.5)(headModel)\n",
        "\n",
        "\t\t# add a softmax layer\n",
        "\t\theadModel = Dense(classes, activation=\"softmax\")(headModel)\n",
        "\n",
        "\t\t# return the model\n",
        "\t\treturn headModel"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bZQdHFwzq7t"
      },
      "source": [
        "Again, this fully-connected head is very simplistic compared to the original head from VGG16 which consists of two sets of 4,096 FC layers. However, for most fine-tuning problems you are not seeking to replicate the original head of the network, but rather simplify it so it is easier to fine-tune– the fewer parameters in the head, the more likely we’ll be to correctly tune the network to a new\n",
        "classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgmBX1ST1F8U"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "class ImageToArrayPreprocessor:\n",
        "\tdef __init__(self, dataFormat=None):\n",
        "\t\t# store the image data format\n",
        "\t\tself.dataFormat = dataFormat\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# apply the Keras utility function that correctly rearranges\n",
        "\t\t# the dimensions of the image\n",
        "\t\treturn img_to_array(image, data_format=self.dataFormat)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ga-FgPH1NuW"
      },
      "source": [
        "# import the necessary packages\n",
        "import imutils\n",
        "import cv2\n",
        "\n",
        "# useful class to help the resize of images\n",
        "class AspectAwarePreprocessor:\n",
        "\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n",
        "\t\t# store the target image width, height, and interpolation\n",
        "\t\t# method used when resizing\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\t\tself.inter = inter\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# grab the dimensions of the image and then initialize\n",
        "\t\t# the deltas to use when cropping\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\t\tdW = 0\n",
        "\t\tdH = 0\n",
        "\n",
        "\t\t# if the width is smaller than the height, then resize\n",
        "\t\t# along the width (i.e., the smaller dimension) and then\n",
        "\t\t# update the deltas to crop the height to the desired\n",
        "\t\t# dimension\n",
        "\t\tif w < h:\n",
        "\t\t\timage = imutils.resize(image, width=self.width,\n",
        "\t\t\t\tinter=self.inter)\n",
        "\t\t\tdH = int((image.shape[0] - self.height) / 2.0)\n",
        "\n",
        "\t\t# otherwise, the height is smaller than the width so\n",
        "\t\t# resize along the height and then update the deltas\n",
        "\t\t# crop along the width\n",
        "\t\telse:\n",
        "\t\t\timage = imutils.resize(image, height=self.height,\n",
        "\t\t\t\tinter=self.inter)\n",
        "\t\t\tdW = int((image.shape[1] - self.width) / 2.0)\n",
        "\n",
        "\t\t# now that our images have been resized, we need to\n",
        "\t\t# re-grab the width and height, followed by performing\n",
        "\t\t# the crop\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\t\timage = image[dH:h - dH, dW:w - dW]\n",
        "\n",
        "\t\t# finally, resize the image to the provided spatial\n",
        "\t\t# dimensions to ensure our output image is always a fixed\n",
        "\t\t# size\n",
        "\t\treturn cv2.resize(image, (self.width, self.height),\n",
        "\t\t\tinterpolation=self.inter)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYq8Q1S71sxt"
      },
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# helper to load images\n",
        "class SimpleDatasetLoader:\n",
        "\tdef __init__(self, preprocessors=None):\n",
        "\t\t# store the image preprocessor\n",
        "\t\tself.preprocessors = preprocessors\n",
        "\n",
        "\t\t# if the preprocessors are None, initialize them as an\n",
        "\t\t# empty list\n",
        "\t\tif self.preprocessors is None:\n",
        "\t\t\tself.preprocessors = []\n",
        "\n",
        "\tdef load(self, imagePaths, verbose=-1):\n",
        "\t\t# initialize the list of features and labels\n",
        "\t\tdata = []\n",
        "\t\tlabels = []\n",
        "\n",
        "\t\t# loop over the input images\n",
        "\t\tfor (i, imagePath) in enumerate(imagePaths):\n",
        "\t\t\t# load the image and extract the class label assuming\n",
        "\t\t\t# that our path has the following format:\n",
        "\t\t\t# /path/to/dataset/{class}/{image}.jpg\n",
        "\t\t\timage = cv2.imread(imagePath)\n",
        "\t\t\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "\t\t\t# check to see if our preprocessors are not None\n",
        "\t\t\tif self.preprocessors is not None:\n",
        "\t\t\t\t# loop over the preprocessors and apply each to\n",
        "\t\t\t\t# the image\n",
        "\t\t\t\tfor p in self.preprocessors:\n",
        "\t\t\t\t\timage = p.preprocess(image)\n",
        "\n",
        "\t\t\t# treat our processed image as a \"feature vector\"\n",
        "\t\t\t# by updating the data list followed by the labels\n",
        "\t\t\tdata.append(image)\n",
        "\t\t\tlabels.append(label)\n",
        "\n",
        "\t\t\t# show an update every `verbose` images\n",
        "\t\t\tif verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
        "\t\t\t\tprint(\"[INFO] processed {}/{}\".format(i + 1,\n",
        "\t\t\t\t\tlen(imagePaths)))\n",
        "\n",
        "\t\t# return a tuple of the data and labels\n",
        "\t\treturn (np.array(data), np.array(labels))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VTRu6F82ZY1"
      },
      "source": [
        "In some cases you’ll want to allow the entire body to be trainable; however, for deeper architectures with many parameters such as VGG, I suggest only unfreezing the top CONV layers and then continuing training. If classification accuracy continues to improve (without overfitting), you may want to consider unfreezing more layers in the body."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pZDxBAl0j3j",
        "outputId": "97fe8f4d-6b7a-43dd-ed78-cb1bb5991a17"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# \"path to input dataset\"\n",
        "dataset = \"flowers17\"\n",
        "\n",
        "# output model\n",
        "model_out = \"flowers17.model\"\n",
        "\n",
        "\n",
        "# construct the image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
        "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
        "                         horizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "# grab the list of images that we'll be describing, then extract\n",
        "# the class label names from the image paths\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = list(paths.list_images(dataset))\n",
        "classNames = [pt.split(os.path.sep)[-2] for pt in imagePaths]\n",
        "classNames = [str(x) for x in np.unique(classNames)]\n",
        "\n",
        "# initialize the image preprocessors\n",
        "aap = AspectAwarePreprocessor(224, 224)\n",
        "iap = ImageToArrayPreprocessor()\n",
        "\n",
        "# load the dataset from disk then scale the raw pixel intensities to\n",
        "# the range [0, 1]\n",
        "sdl = SimpleDatasetLoader(preprocessors=[aap, iap])\n",
        "(data, labels) = sdl.load(imagePaths, verbose=500)\n",
        "data = data.astype(\"float\") / 255.0\n",
        "\n",
        "# partition the data into training and testing splits using 75% of\n",
        "# the data for training and the remaining 25% for testing\n",
        "(train_x, test_x, train_y, test_y) = train_test_split(data, labels,\n",
        "                                                    test_size=0.25, \n",
        "                                                    random_state=42)\n",
        "\n",
        "# convert the labels from integers to vectors\n",
        "train_y = LabelBinarizer().fit_transform(train_y)\n",
        "test_y = LabelBinarizer().fit_transform(test_y)\n",
        "\n",
        "# load the VGG16 network, ensuring the head FC layer sets are left\n",
        "# off\n",
        "baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
        "                  input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# initialize the new head of the network, a set of FC layers\n",
        "# followed by a softmax classifier\n",
        "headModel = FCHeadNet.build(baseModel, len(classNames), 256)\n",
        "\n",
        "# place the head FC model on top of the base model -- this will\n",
        "# become the actual model we will train\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "# loop over all layers in the base model and freeze them so they\n",
        "# will *not* be updated during the training process\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False\n",
        "\n",
        "# compile our model (this needs to be done after our setting our\n",
        "# layers to being non-trainable\n",
        "print(\"[INFO] compiling model...\")\n",
        "\n",
        "\n",
        "# RMSprop is frequently used in situations where we need to quickly obtain\n",
        "# reasonable performance (as is the case when we are trying to “warm up” a set of FC layers).\n",
        "opt = RMSprop(learning_rate=0.001)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# train the head of the network for a few epochs (all other\n",
        "# layers are frozen) -- this will allow the new FC layers to\n",
        "# start to become initialized with actual \"learned\" values\n",
        "# versus pure random\n",
        "print(\"[INFO] training head...\")\n",
        "model.fit(aug.flow(train_x, train_y, batch_size=32),\n",
        "                    validation_data=(test_x, test_y), epochs=25,\n",
        "                    steps_per_epoch=len(train_x) // 32, verbose=1)\n",
        "\n",
        "# evaluate the network after initialization\n",
        "print(\"[INFO] evaluating after initialization...\")\n",
        "predictions = model.predict(test_x, batch_size=32)\n",
        "print(classification_report(test_y.argmax(axis=1),\n",
        "                            predictions.argmax(axis=1), target_names=classNames))\n",
        "\n",
        "# now that the head FC layers have been trained/initialized, lets\n",
        "# unfreeze the final set of CONV layers and make them trainable\n",
        "for layer in baseModel.layers[15:]:\n",
        "\tlayer.trainable = True\n",
        "\n",
        "# for the changes to the model to take affect we need to recompile\n",
        "# the model, this time using SGD with a *very* small learning rate\n",
        "print(\"[INFO] re-compiling model...\")\n",
        "opt = SGD(learning_rate=0.001)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "# train the model again, this time fine-tuning *both* the final set\n",
        "# of CONV layers along with our set of FC layers\n",
        "print(\"[INFO] fine-tuning model...\")\n",
        "model.fit(aug.flow(train_x, train_y, batch_size=32),\n",
        "          validation_data=(test_x, test_y), epochs=100,\n",
        "          steps_per_epoch=len(train_x) // 32, verbose=1)\n",
        "\n",
        "# evaluate the network on the fine-tuned model\n",
        "print(\"[INFO] evaluating after fine-tuning...\")\n",
        "predictions = model.predict(test_x, batch_size=32)\n",
        "print(classification_report(test_y.argmax(axis=1),\n",
        "                            predictions.argmax(axis=1), target_names=classNames))\n",
        "\n",
        "# save the model to disk\n",
        "print(\"[INFO] serializing model...\")\n",
        "model.save(model_out)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n",
            "[INFO] processed 500/1360\n",
            "[INFO] processed 1000/1360\n",
            "[INFO] compiling model...\n",
            "[INFO] training head...\n",
            "Epoch 1/25\n",
            "31/31 [==============================] - 10s 309ms/step - loss: 5.0210 - accuracy: 0.1812 - val_loss: 2.2655 - val_accuracy: 0.3471\n",
            "Epoch 2/25\n",
            "31/31 [==============================] - 9s 303ms/step - loss: 2.3500 - accuracy: 0.2885 - val_loss: 1.7931 - val_accuracy: 0.4029\n",
            "Epoch 3/25\n",
            "31/31 [==============================] - 9s 299ms/step - loss: 1.9352 - accuracy: 0.3877 - val_loss: 1.2597 - val_accuracy: 0.6500\n",
            "Epoch 4/25\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 1.6348 - accuracy: 0.4949 - val_loss: 1.0033 - val_accuracy: 0.7353\n",
            "Epoch 5/25\n",
            "31/31 [==============================] - 9s 300ms/step - loss: 1.5265 - accuracy: 0.5304 - val_loss: 0.8990 - val_accuracy: 0.7559\n",
            "Epoch 6/25\n",
            "31/31 [==============================] - 9s 298ms/step - loss: 1.2890 - accuracy: 0.5992 - val_loss: 0.9668 - val_accuracy: 0.6735\n",
            "Epoch 7/25\n",
            "31/31 [==============================] - 9s 299ms/step - loss: 1.3058 - accuracy: 0.6002 - val_loss: 0.6731 - val_accuracy: 0.8059\n",
            "Epoch 8/25\n",
            "31/31 [==============================] - 9s 295ms/step - loss: 1.1371 - accuracy: 0.6447 - val_loss: 0.5670 - val_accuracy: 0.8441\n",
            "Epoch 9/25\n",
            "31/31 [==============================] - 9s 303ms/step - loss: 1.1002 - accuracy: 0.6498 - val_loss: 0.5992 - val_accuracy: 0.8471\n",
            "Epoch 10/25\n",
            "31/31 [==============================] - 9s 298ms/step - loss: 1.0373 - accuracy: 0.6650 - val_loss: 0.5667 - val_accuracy: 0.8206\n",
            "Epoch 11/25\n",
            "31/31 [==============================] - 9s 298ms/step - loss: 0.9293 - accuracy: 0.7167 - val_loss: 0.6899 - val_accuracy: 0.7735\n",
            "Epoch 12/25\n",
            "31/31 [==============================] - 9s 298ms/step - loss: 0.9453 - accuracy: 0.7115 - val_loss: 0.5458 - val_accuracy: 0.8353\n",
            "Epoch 13/25\n",
            "31/31 [==============================] - 9s 299ms/step - loss: 0.8394 - accuracy: 0.7267 - val_loss: 0.5798 - val_accuracy: 0.8147\n",
            "Epoch 14/25\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.8478 - accuracy: 0.7146 - val_loss: 0.4347 - val_accuracy: 0.8676\n",
            "Epoch 15/25\n",
            "31/31 [==============================] - 9s 298ms/step - loss: 0.8003 - accuracy: 0.7237 - val_loss: 0.4905 - val_accuracy: 0.8500\n",
            "Epoch 16/25\n",
            "31/31 [==============================] - 9s 300ms/step - loss: 0.7836 - accuracy: 0.7460 - val_loss: 0.5086 - val_accuracy: 0.8324\n",
            "Epoch 17/25\n",
            "31/31 [==============================] - 9s 299ms/step - loss: 0.7739 - accuracy: 0.7652 - val_loss: 0.4135 - val_accuracy: 0.8853\n",
            "Epoch 18/25\n",
            "31/31 [==============================] - 9s 295ms/step - loss: 0.7516 - accuracy: 0.7591 - val_loss: 0.4616 - val_accuracy: 0.8588\n",
            "Epoch 19/25\n",
            "31/31 [==============================] - 9s 300ms/step - loss: 0.7417 - accuracy: 0.7824 - val_loss: 0.5574 - val_accuracy: 0.8441\n",
            "Epoch 20/25\n",
            "31/31 [==============================] - 9s 300ms/step - loss: 0.6788 - accuracy: 0.7804 - val_loss: 0.3927 - val_accuracy: 0.8765\n",
            "Epoch 21/25\n",
            "31/31 [==============================] - 9s 298ms/step - loss: 0.6761 - accuracy: 0.7895 - val_loss: 0.4957 - val_accuracy: 0.8529\n",
            "Epoch 22/25\n",
            "31/31 [==============================] - 9s 303ms/step - loss: 0.7041 - accuracy: 0.7874 - val_loss: 0.4221 - val_accuracy: 0.8618\n",
            "Epoch 23/25\n",
            "31/31 [==============================] - 9s 302ms/step - loss: 0.6123 - accuracy: 0.7794 - val_loss: 0.4031 - val_accuracy: 0.8941\n",
            "Epoch 24/25\n",
            "31/31 [==============================] - 9s 301ms/step - loss: 0.6036 - accuracy: 0.8107 - val_loss: 0.4648 - val_accuracy: 0.8735\n",
            "Epoch 25/25\n",
            "31/31 [==============================] - 9s 301ms/step - loss: 0.6230 - accuracy: 0.7925 - val_loss: 0.3752 - val_accuracy: 0.8824\n",
            "[INFO] evaluating after initialization...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    bluebell       0.94      0.79      0.86        19\n",
            "   buttercup       0.90      0.95      0.92        19\n",
            "   coltsfoot       0.79      0.94      0.86        16\n",
            "     cowslip       0.77      0.85      0.81        20\n",
            "      crocus       0.73      0.89      0.80        18\n",
            "    daffodil       0.78      0.78      0.78        23\n",
            "       daisy       1.00      0.95      0.97        20\n",
            "   dandelion       0.94      0.80      0.86        20\n",
            "  fritillary       1.00      0.86      0.92        21\n",
            "        iris       1.00      1.00      1.00        16\n",
            "  lilyvalley       0.95      0.95      0.95        22\n",
            "       pansy       1.00      0.91      0.95        23\n",
            "    snowdrop       0.95      0.83      0.88        23\n",
            "   sunflower       0.95      0.95      0.95        20\n",
            "   tigerlily       0.82      0.93      0.87        15\n",
            "       tulip       0.69      0.74      0.71        27\n",
            "  windflower       0.95      1.00      0.97        18\n",
            "\n",
            "    accuracy                           0.88       340\n",
            "   macro avg       0.89      0.89      0.89       340\n",
            "weighted avg       0.89      0.88      0.88       340\n",
            "\n",
            "[INFO] re-compiling model...\n",
            "[INFO] fine-tuning model...\n",
            "Epoch 1/100\n",
            "31/31 [==============================] - 10s 296ms/step - loss: 0.4243 - accuracy: 0.8532 - val_loss: 0.3440 - val_accuracy: 0.9000\n",
            "Epoch 2/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.3763 - accuracy: 0.8735 - val_loss: 0.2983 - val_accuracy: 0.9088\n",
            "Epoch 3/100\n",
            "31/31 [==============================] - 9s 301ms/step - loss: 0.3652 - accuracy: 0.8887 - val_loss: 0.3306 - val_accuracy: 0.9088\n",
            "Epoch 4/100\n",
            "31/31 [==============================] - 9s 299ms/step - loss: 0.3229 - accuracy: 0.8897 - val_loss: 0.3031 - val_accuracy: 0.9000\n",
            "Epoch 5/100\n",
            "31/31 [==============================] - 9s 300ms/step - loss: 0.3254 - accuracy: 0.8947 - val_loss: 0.3119 - val_accuracy: 0.9088\n",
            "Epoch 6/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.3250 - accuracy: 0.8927 - val_loss: 0.2975 - val_accuracy: 0.9088\n",
            "Epoch 7/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.3194 - accuracy: 0.8937 - val_loss: 0.3194 - val_accuracy: 0.9059\n",
            "Epoch 8/100\n",
            "31/31 [==============================] - 9s 295ms/step - loss: 0.2812 - accuracy: 0.8998 - val_loss: 0.2920 - val_accuracy: 0.9147\n",
            "Epoch 9/100\n",
            "31/31 [==============================] - 9s 301ms/step - loss: 0.2977 - accuracy: 0.8917 - val_loss: 0.3133 - val_accuracy: 0.9000\n",
            "Epoch 10/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.2231 - accuracy: 0.9291 - val_loss: 0.3314 - val_accuracy: 0.9029\n",
            "Epoch 11/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.2792 - accuracy: 0.9089 - val_loss: 0.2868 - val_accuracy: 0.9059\n",
            "Epoch 12/100\n",
            "31/31 [==============================] - 9s 298ms/step - loss: 0.2524 - accuracy: 0.9119 - val_loss: 0.2908 - val_accuracy: 0.9147\n",
            "Epoch 13/100\n",
            "31/31 [==============================] - 9s 300ms/step - loss: 0.2321 - accuracy: 0.9271 - val_loss: 0.2823 - val_accuracy: 0.9235\n",
            "Epoch 14/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.2522 - accuracy: 0.9251 - val_loss: 0.2700 - val_accuracy: 0.9176\n",
            "Epoch 15/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.2387 - accuracy: 0.9224 - val_loss: 0.2758 - val_accuracy: 0.9235\n",
            "Epoch 16/100\n",
            "31/31 [==============================] - 9s 300ms/step - loss: 0.2513 - accuracy: 0.9221 - val_loss: 0.2789 - val_accuracy: 0.9176\n",
            "Epoch 17/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.2451 - accuracy: 0.9069 - val_loss: 0.2657 - val_accuracy: 0.9265\n",
            "Epoch 18/100\n",
            "31/31 [==============================] - 9s 294ms/step - loss: 0.2361 - accuracy: 0.9231 - val_loss: 0.2755 - val_accuracy: 0.9206\n",
            "Epoch 19/100\n",
            "31/31 [==============================] - 9s 294ms/step - loss: 0.2409 - accuracy: 0.9221 - val_loss: 0.2568 - val_accuracy: 0.9206\n",
            "Epoch 20/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.2220 - accuracy: 0.9221 - val_loss: 0.3122 - val_accuracy: 0.9147\n",
            "Epoch 21/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.2000 - accuracy: 0.9403 - val_loss: 0.2602 - val_accuracy: 0.9176\n",
            "Epoch 22/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.2262 - accuracy: 0.9241 - val_loss: 0.2484 - val_accuracy: 0.9265\n",
            "Epoch 23/100\n",
            "31/31 [==============================] - 9s 295ms/step - loss: 0.1591 - accuracy: 0.9565 - val_loss: 0.2576 - val_accuracy: 0.9235\n",
            "Epoch 24/100\n",
            "31/31 [==============================] - 9s 292ms/step - loss: 0.2171 - accuracy: 0.9241 - val_loss: 0.2621 - val_accuracy: 0.9265\n",
            "Epoch 25/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.1948 - accuracy: 0.9302 - val_loss: 0.2716 - val_accuracy: 0.9294\n",
            "Epoch 26/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.2034 - accuracy: 0.9241 - val_loss: 0.2796 - val_accuracy: 0.9147\n",
            "Epoch 27/100\n",
            "31/31 [==============================] - 9s 295ms/step - loss: 0.2163 - accuracy: 0.9302 - val_loss: 0.2658 - val_accuracy: 0.9206\n",
            "Epoch 28/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.2121 - accuracy: 0.9281 - val_loss: 0.2575 - val_accuracy: 0.9294\n",
            "Epoch 29/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.2212 - accuracy: 0.9291 - val_loss: 0.2652 - val_accuracy: 0.9235\n",
            "Epoch 30/100\n",
            "31/31 [==============================] - 9s 293ms/step - loss: 0.1859 - accuracy: 0.9312 - val_loss: 0.2825 - val_accuracy: 0.9206\n",
            "Epoch 31/100\n",
            "31/31 [==============================] - 9s 293ms/step - loss: 0.1822 - accuracy: 0.9332 - val_loss: 0.2576 - val_accuracy: 0.9294\n",
            "Epoch 32/100\n",
            "31/31 [==============================] - 9s 295ms/step - loss: 0.1760 - accuracy: 0.9352 - val_loss: 0.2584 - val_accuracy: 0.9176\n",
            "Epoch 33/100\n",
            "31/31 [==============================] - 9s 294ms/step - loss: 0.2007 - accuracy: 0.9251 - val_loss: 0.2620 - val_accuracy: 0.9353\n",
            "Epoch 34/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.1826 - accuracy: 0.9413 - val_loss: 0.2659 - val_accuracy: 0.9353\n",
            "Epoch 35/100\n",
            "31/31 [==============================] - 9s 298ms/step - loss: 0.1655 - accuracy: 0.9484 - val_loss: 0.2601 - val_accuracy: 0.9235\n",
            "Epoch 36/100\n",
            "31/31 [==============================] - 9s 295ms/step - loss: 0.1830 - accuracy: 0.9453 - val_loss: 0.2602 - val_accuracy: 0.9176\n",
            "Epoch 37/100\n",
            "31/31 [==============================] - 9s 299ms/step - loss: 0.1907 - accuracy: 0.9403 - val_loss: 0.2670 - val_accuracy: 0.9206\n",
            "Epoch 38/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.1668 - accuracy: 0.9453 - val_loss: 0.2508 - val_accuracy: 0.9353\n",
            "Epoch 39/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.1752 - accuracy: 0.9413 - val_loss: 0.2823 - val_accuracy: 0.9147\n",
            "Epoch 40/100\n",
            "31/31 [==============================] - 9s 293ms/step - loss: 0.1908 - accuracy: 0.9281 - val_loss: 0.2547 - val_accuracy: 0.9324\n",
            "Epoch 41/100\n",
            "31/31 [==============================] - 9s 294ms/step - loss: 0.1784 - accuracy: 0.9403 - val_loss: 0.2779 - val_accuracy: 0.9265\n",
            "Epoch 42/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.1734 - accuracy: 0.9372 - val_loss: 0.2774 - val_accuracy: 0.9206\n",
            "Epoch 43/100\n",
            "31/31 [==============================] - 9s 292ms/step - loss: 0.1424 - accuracy: 0.9504 - val_loss: 0.2565 - val_accuracy: 0.9353\n",
            "Epoch 44/100\n",
            "31/31 [==============================] - 9s 290ms/step - loss: 0.1837 - accuracy: 0.9332 - val_loss: 0.2534 - val_accuracy: 0.9353\n",
            "Epoch 45/100\n",
            "31/31 [==============================] - 9s 295ms/step - loss: 0.1860 - accuracy: 0.9393 - val_loss: 0.2600 - val_accuracy: 0.9294\n",
            "Epoch 46/100\n",
            "31/31 [==============================] - 9s 294ms/step - loss: 0.1689 - accuracy: 0.9443 - val_loss: 0.2304 - val_accuracy: 0.9441\n",
            "Epoch 47/100\n",
            "31/31 [==============================] - 9s 299ms/step - loss: 0.1694 - accuracy: 0.9524 - val_loss: 0.2536 - val_accuracy: 0.9294\n",
            "Epoch 48/100\n",
            "31/31 [==============================] - 9s 299ms/step - loss: 0.1174 - accuracy: 0.9534 - val_loss: 0.2787 - val_accuracy: 0.9294\n",
            "Epoch 49/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.1458 - accuracy: 0.9514 - val_loss: 0.2498 - val_accuracy: 0.9294\n",
            "Epoch 50/100\n",
            "31/31 [==============================] - 9s 293ms/step - loss: 0.1466 - accuracy: 0.9545 - val_loss: 0.2750 - val_accuracy: 0.9206\n",
            "Epoch 51/100\n",
            "31/31 [==============================] - 9s 300ms/step - loss: 0.1594 - accuracy: 0.9494 - val_loss: 0.2699 - val_accuracy: 0.9206\n",
            "Epoch 52/100\n",
            "31/31 [==============================] - 9s 302ms/step - loss: 0.1404 - accuracy: 0.9474 - val_loss: 0.2466 - val_accuracy: 0.9382\n",
            "Epoch 53/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.1751 - accuracy: 0.9474 - val_loss: 0.2377 - val_accuracy: 0.9294\n",
            "Epoch 54/100\n",
            "31/31 [==============================] - 9s 294ms/step - loss: 0.1496 - accuracy: 0.9504 - val_loss: 0.2550 - val_accuracy: 0.9294\n",
            "Epoch 55/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.1544 - accuracy: 0.9464 - val_loss: 0.2435 - val_accuracy: 0.9353\n",
            "Epoch 56/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.1299 - accuracy: 0.9494 - val_loss: 0.2387 - val_accuracy: 0.9353\n",
            "Epoch 57/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.1275 - accuracy: 0.9585 - val_loss: 0.2637 - val_accuracy: 0.9294\n",
            "Epoch 58/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.1260 - accuracy: 0.9575 - val_loss: 0.2311 - val_accuracy: 0.9294\n",
            "Epoch 59/100\n",
            "31/31 [==============================] - 9s 299ms/step - loss: 0.1249 - accuracy: 0.9565 - val_loss: 0.2653 - val_accuracy: 0.9265\n",
            "Epoch 60/100\n",
            "31/31 [==============================] - 9s 293ms/step - loss: 0.1503 - accuracy: 0.9474 - val_loss: 0.2628 - val_accuracy: 0.9294\n",
            "Epoch 61/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.1088 - accuracy: 0.9615 - val_loss: 0.2785 - val_accuracy: 0.9118\n",
            "Epoch 62/100\n",
            "31/31 [==============================] - 9s 294ms/step - loss: 0.1509 - accuracy: 0.9443 - val_loss: 0.2574 - val_accuracy: 0.9294\n",
            "Epoch 63/100\n",
            "31/31 [==============================] - 9s 294ms/step - loss: 0.1370 - accuracy: 0.9565 - val_loss: 0.2407 - val_accuracy: 0.9324\n",
            "Epoch 64/100\n",
            "31/31 [==============================] - 9s 295ms/step - loss: 0.1263 - accuracy: 0.9626 - val_loss: 0.2703 - val_accuracy: 0.9324\n",
            "Epoch 65/100\n",
            "31/31 [==============================] - 9s 294ms/step - loss: 0.1406 - accuracy: 0.9484 - val_loss: 0.2712 - val_accuracy: 0.9294\n",
            "Epoch 66/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.1274 - accuracy: 0.9524 - val_loss: 0.2921 - val_accuracy: 0.9206\n",
            "Epoch 67/100\n",
            "31/31 [==============================] - 9s 299ms/step - loss: 0.1198 - accuracy: 0.9605 - val_loss: 0.2691 - val_accuracy: 0.9206\n",
            "Epoch 68/100\n",
            "31/31 [==============================] - 9s 294ms/step - loss: 0.1207 - accuracy: 0.9636 - val_loss: 0.2386 - val_accuracy: 0.9353\n",
            "Epoch 69/100\n",
            "31/31 [==============================] - 9s 294ms/step - loss: 0.1504 - accuracy: 0.9464 - val_loss: 0.2555 - val_accuracy: 0.9265\n",
            "Epoch 70/100\n",
            "31/31 [==============================] - 9s 295ms/step - loss: 0.1455 - accuracy: 0.9504 - val_loss: 0.2721 - val_accuracy: 0.9206\n",
            "Epoch 71/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.1442 - accuracy: 0.9534 - val_loss: 0.2314 - val_accuracy: 0.9441\n",
            "Epoch 72/100\n",
            "31/31 [==============================] - 9s 302ms/step - loss: 0.0971 - accuracy: 0.9696 - val_loss: 0.2734 - val_accuracy: 0.9324\n",
            "Epoch 73/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.1438 - accuracy: 0.9555 - val_loss: 0.2389 - val_accuracy: 0.9324\n",
            "Epoch 74/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.1103 - accuracy: 0.9656 - val_loss: 0.2674 - val_accuracy: 0.9324\n",
            "Epoch 75/100\n",
            "31/31 [==============================] - 9s 294ms/step - loss: 0.1424 - accuracy: 0.9595 - val_loss: 0.2757 - val_accuracy: 0.9206\n",
            "Epoch 76/100\n",
            "31/31 [==============================] - 9s 293ms/step - loss: 0.1284 - accuracy: 0.9545 - val_loss: 0.2765 - val_accuracy: 0.9294\n",
            "Epoch 77/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.1241 - accuracy: 0.9555 - val_loss: 0.2576 - val_accuracy: 0.9324\n",
            "Epoch 78/100\n",
            "31/31 [==============================] - 9s 293ms/step - loss: 0.1051 - accuracy: 0.9686 - val_loss: 0.2718 - val_accuracy: 0.9324\n",
            "Epoch 79/100\n",
            "31/31 [==============================] - 9s 293ms/step - loss: 0.1233 - accuracy: 0.9615 - val_loss: 0.2639 - val_accuracy: 0.9353\n",
            "Epoch 80/100\n",
            "31/31 [==============================] - 9s 298ms/step - loss: 0.1347 - accuracy: 0.9545 - val_loss: 0.2496 - val_accuracy: 0.9324\n",
            "Epoch 81/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.1193 - accuracy: 0.9595 - val_loss: 0.2515 - val_accuracy: 0.9412\n",
            "Epoch 82/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.1049 - accuracy: 0.9686 - val_loss: 0.2382 - val_accuracy: 0.9412\n",
            "Epoch 83/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.1137 - accuracy: 0.9666 - val_loss: 0.2704 - val_accuracy: 0.9324\n",
            "Epoch 84/100\n",
            "31/31 [==============================] - 9s 299ms/step - loss: 0.1057 - accuracy: 0.9656 - val_loss: 0.2978 - val_accuracy: 0.9324\n",
            "Epoch 85/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.0933 - accuracy: 0.9717 - val_loss: 0.2994 - val_accuracy: 0.9353\n",
            "Epoch 86/100\n",
            "31/31 [==============================] - 9s 295ms/step - loss: 0.1125 - accuracy: 0.9615 - val_loss: 0.2683 - val_accuracy: 0.9353\n",
            "Epoch 87/100\n",
            "31/31 [==============================] - 9s 300ms/step - loss: 0.1179 - accuracy: 0.9585 - val_loss: 0.2549 - val_accuracy: 0.9412\n",
            "Epoch 88/100\n",
            "31/31 [==============================] - 9s 295ms/step - loss: 0.0881 - accuracy: 0.9787 - val_loss: 0.2765 - val_accuracy: 0.9412\n",
            "Epoch 89/100\n",
            "31/31 [==============================] - 9s 294ms/step - loss: 0.1239 - accuracy: 0.9545 - val_loss: 0.2734 - val_accuracy: 0.9324\n",
            "Epoch 90/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.0990 - accuracy: 0.9706 - val_loss: 0.2727 - val_accuracy: 0.9412\n",
            "Epoch 91/100\n",
            "31/31 [==============================] - 9s 293ms/step - loss: 0.0972 - accuracy: 0.9666 - val_loss: 0.2738 - val_accuracy: 0.9382\n",
            "Epoch 92/100\n",
            "31/31 [==============================] - 9s 294ms/step - loss: 0.1234 - accuracy: 0.9615 - val_loss: 0.2538 - val_accuracy: 0.9382\n",
            "Epoch 93/100\n",
            "31/31 [==============================] - 9s 293ms/step - loss: 0.0899 - accuracy: 0.9727 - val_loss: 0.2702 - val_accuracy: 0.9412\n",
            "Epoch 94/100\n",
            "31/31 [==============================] - 9s 297ms/step - loss: 0.0904 - accuracy: 0.9686 - val_loss: 0.2923 - val_accuracy: 0.9353\n",
            "Epoch 95/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.0984 - accuracy: 0.9696 - val_loss: 0.2957 - val_accuracy: 0.9353\n",
            "Epoch 96/100\n",
            "31/31 [==============================] - 9s 293ms/step - loss: 0.1055 - accuracy: 0.9676 - val_loss: 0.2601 - val_accuracy: 0.9353\n",
            "Epoch 97/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.0950 - accuracy: 0.9747 - val_loss: 0.2944 - val_accuracy: 0.9412\n",
            "Epoch 98/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.1145 - accuracy: 0.9626 - val_loss: 0.2467 - val_accuracy: 0.9382\n",
            "Epoch 99/100\n",
            "31/31 [==============================] - 9s 295ms/step - loss: 0.1046 - accuracy: 0.9737 - val_loss: 0.2891 - val_accuracy: 0.9294\n",
            "Epoch 100/100\n",
            "31/31 [==============================] - 9s 296ms/step - loss: 0.1149 - accuracy: 0.9615 - val_loss: 0.2585 - val_accuracy: 0.9353\n",
            "[INFO] evaluating after fine-tuning...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    bluebell       0.90      0.95      0.92        19\n",
            "   buttercup       1.00      0.95      0.97        19\n",
            "   coltsfoot       0.83      0.94      0.88        16\n",
            "     cowslip       0.90      0.90      0.90        20\n",
            "      crocus       0.85      0.94      0.89        18\n",
            "    daffodil       0.91      0.87      0.89        23\n",
            "       daisy       1.00      0.95      0.97        20\n",
            "   dandelion       0.89      0.85      0.87        20\n",
            "  fritillary       1.00      0.90      0.95        21\n",
            "        iris       1.00      1.00      1.00        16\n",
            "  lilyvalley       1.00      0.95      0.98        22\n",
            "       pansy       1.00      0.91      0.95        23\n",
            "    snowdrop       0.92      1.00      0.96        23\n",
            "   sunflower       1.00      0.90      0.95        20\n",
            "   tigerlily       1.00      1.00      1.00        15\n",
            "       tulip       0.81      0.93      0.86        27\n",
            "  windflower       1.00      1.00      1.00        18\n",
            "\n",
            "    accuracy                           0.94       340\n",
            "   macro avg       0.94      0.94      0.94       340\n",
            "weighted avg       0.94      0.94      0.94       340\n",
            "\n",
            "[INFO] serializing model...\n",
            "INFO:tensorflow:Assets written to: flowers17.model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sQqnorJ2Uk_"
      },
      "source": [
        "Additional accuracy can be obtained by performing more aggressive data augmentation and continually unfreezing more and more CONV blocks in VGG16. While fine-tuning is certainly more work than feature extraction, it also enables us to tune and modify the weights in our CNN to a particular dataset – something that feature extraction does not allow. Thus, when given enough training data, consider applying fine-tuning as you’ll likely obtain higher classification accuracy\n",
        "than simple feature extraction alone."
      ]
    }
  ]
}