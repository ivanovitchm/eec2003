{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson #06.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3HMLF0xjFuN"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iQkw0f8hVTs"
      },
      "source": [
        "In this lesson you will discover the architectural innovations in the development and use of convolutional neural networks. After reading the next sections, you will know:\n",
        "\n",
        "- The **ImageNet Large Scale Visual Recognition Challenge** (ILSVRC) and the deep learning-based innovations it has helped to bring about.\n",
        "\n",
        "- The **key architectural milestones** in the development and use of convolutional neural networks over the last two decades (Working in Progressing). In this first lesson will see only LeNet-5 and AlexNet, later other models will be presented\n",
        "\n",
        "- Working with HDFS files and large datasets using Cat & Dogs competition.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoxOz149h-S5"
      },
      "source": [
        "# 1.0 ImageNet, ILSVRC, and Milestone Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koM7HJtijOGO"
      },
      "source": [
        "The rise in popularity and use of deep learning neural network techniques can be traced back to the innovations in applying convolutional neural networks to image classification tasks. Some of the most important innovations have sprung from submissions by academics and industry leaders to the [ImageNet Large Scale Visual Recognition Challenge](http://image-net.org/challenges/LSVRC/), or **ILSVRC**.\n",
        "\n",
        "The ILSVRC is an annual computer vision competition developed upon a subset of a publicly available computer vision dataset called **ImageNet**. As such, the tasks and even the challenge itself are often referred to as the ImageNet Competition. In this section, you will discover the **ImageNet dataset**, the ILSVRC, and the key milestones in image classification that have resulted from the competitions. After reading this section, you will know:\n",
        "\n",
        "- The ImageNet dataset is a huge collection of human-annotated photographs designed by academics for developing computer vision algorithms.\n",
        "- The ImageNet Large Scale Visual Recognition Challenge, or ILSVRC, is an annual competition that uses subsets from the ImageNet dataset and is designed to foster the development and benchmarking of state-of-the-art algorithms.\n",
        "- The ILSVRC tasks have led to milestone model architectures and techniques in intersection of computer vision and deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf3egQYnldra"
      },
      "source": [
        "## 1.1 ImageNet Dataset & ILSVRC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6J6v4SdlyBn"
      },
      "source": [
        "**ImageNet** is a large dataset of annotated photographs intended for computer vision research. The goal of developing the dataset was to provide a resource to promote improved research and development methods for computer vision.\n",
        "\n",
        "Based on statistics about the dataset recorded on the [ImageNet homepage](http://www.image-net.org/index), there are a little more than **14 million images** in the dataset, a little more than **21 thousand groups** or classes (called synsets), and a little more than **1 million images that have bounding box annotations** (e.g.\n",
        "boxes around identified objects in the images). \n",
        "\n",
        "- The photographs were annotated by humans using crowdsourcing platforms such as [Amazon’s Mechanical Turk](https://www.mturk.com/). \n",
        "- The project to develop and maintain the dataset was organized and executed by a collocation between academics at [Princeton](https://www.cs.princeton.edu/~kaiyuy/), [Stanford](https://profiles.stanford.edu/fei-fei-li), and other American universities. \n",
        "- The project does not own the photographs that make up the images; instead, they are owned by the copyright holders. As such, the dataset is not distributed directly; URLs are provided to the images included in the dataset.\n",
        "\n",
        "\n",
        "The **ImageNet Large Scale Visual Recognition Challenge**, or ILSVRC for short, is an annual computer vision competition held to challenge tasks to use subsets of the ImageNet dataset. The challenge was to both promote the development of better computer vision techniques and benchmark the state-of-the-art. The annual challenge focuses on multiple tasks for image classification that includes both assigning a class label to an image based on the main object in the photograph and object detection that involves localizing objects within the photograph.\n",
        "\n",
        "The general challenge tasks for most years are as follows:\n",
        "\n",
        "- **Image classification**: Predict the classes of objects present in an image.\n",
        "- **Single-object localization**: Image classification + draw a bounding box around one example of each object present.\n",
        "- **Object detection**: Image classification + draw a bounding box around each object present.\n",
        "\n",
        "More recently, and given the great success in developing techniques for still photographs, the challenge tasks are changing to more complicated tasks such as labeling videos. \n",
        "\n",
        "> The datasets comprised approximately **1 million images** and **1,000 object classes**. \n",
        "\n",
        "The datasets used in challenge tasks are sometimes varied (depending on the task) and were released publicly to promote widespread participation from academia and industry. For each annual challenge, an annotated training dataset was released, along with an unannotated test dataset for which annotations had to be made and submitted to a server for evaluation. Typically, **the training dataset** was comprised of **1 million images**, with **50,000 for a validation** dataset and **150,000 for a test dataset**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqCzEcmZn-Z5"
      },
      "source": [
        "## 1.2 Deep Learning Milestones From ILSVRC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTRhQe5nLoB2"
      },
      "source": [
        "Researchers working on ILSVRC tasks have pushed back the frontier of computer vision research. The methods and papers that describe them are milestones in computer vision, deep learning, and, more broadly in artificial intelligence. The pace of improvement in the first five years of the ILSVRC was dramatic, perhaps even shocking to the field of computer vision. **Success has primarily been achieved by large (deep) convolutional neural networks (CNNs) on graphical processing unit (GPU) hardware**, which sparked an interest in deep learning that extended beyond the field out into the mainstream.\n",
        "\n",
        "> State-of-the-art accuracy has improved significantly from ILSVRC2010 to ILSVRC2014, showcasing the massive progress that has been made in large-scale object recognition over the past five years.\n",
        "\n",
        "There has been widespread participation in the ILSVRC over the years, with many significant developments and enormous academic publications. Picking out milestones from so much work is a challenge in and of itself. Nevertheless, there are techniques, often named for their parent university, research group, or company, that stand out and have become staples in the intersecting fields of deep learning and computer vision. The papers that describe the methods have become required reading, and the techniques used by the models have become\n",
        "heuristics when using general techniques in practice.\n",
        "\n",
        "> This section will highlight some of these milestone techniques proposed as part of ILSVRC in which they were introduced and the papers that describe them. The focus will be on image classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VvBnv71O-yJ"
      },
      "source": [
        "**Alexnet**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODbUIjpGOru1"
      },
      "source": [
        "Alex Krizhevsky, et al. from the University of Toronto in their 2012 paper titled [ImageNet\n",
        "Classification with Deep Convolutional Neural Networks](https://www.cs.toronto.edu/~kriz/imagenet_classification_with_deep_convolutional.pdf) developed a convolutional neural network\n",
        "that achieved top results on the ILSVRC-2010 and ILSVRC-2012 image classification tasks.\n",
        "These results sparked interest in deep learning in computer vision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq-jrZ_fO8pq"
      },
      "source": [
        "**ZFNet**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ckx6IUrBPmPE"
      },
      "source": [
        "Matthew Zeiler and Rob Fergus proposed a variation of AlexNet generally referred to as ZFNet\n",
        "in their 2013 paper titled [Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901), a variation of which won the ILSVRC-2013 image classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyOU3OcAP2TW"
      },
      "source": [
        "**Incepiton (GoogLeNet)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtGPBobiP9WT"
      },
      "source": [
        "Christian Szegedy, et al. from Google achieved top results for object detection with their\n",
        "GoogLeNet model that made use of the inception module and architecture. This approach was\n",
        "described in their 2014 paper titled [Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1bg23MWQhrO"
      },
      "source": [
        "**VGG**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RvBdRb8Qr-T"
      },
      "source": [
        "Karen Simonyan and Andrew Zisserman from the Oxford Vision Geometry Group (VGG)\n",
        "achieved top results for image classification and localization with their VGG model. Their\n",
        "approach is described in their 2015 paper titled [Very Deep Convolutional Networks for Large-\n",
        "Scale Image Recognition](https://arxiv.org/abs/1409.1556)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h46pqhv7QtuB"
      },
      "source": [
        "**ResNet**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT7H9gFORAR-"
      },
      "source": [
        "Kaiming He, et al. from Microsoft Research achieved top results for object detection and object detection with localization tasks with their Residual Network or ResNet described in their 2015 paper titled [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMJtZNR5RSAW"
      },
      "source": [
        "# 2.0 How Milestone Model Architectural Innovations Work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW6k_AmlWGEu"
      },
      "source": [
        "Convolutional neural networks are comprised of two straightforward elements, namely **convolutional layers** and **pooling layers**. Although simple, there are near-infinite ways to arrange these layers for a given computer vision problem. Fortunately, **there are both common patterns** for\n",
        "configuring these layers and architectural innovations that you can use in order to develop very deep convolutional neural networks. \n",
        "\n",
        "> Studying these architectural design decisions developed for state-of-the-art image classification tasks can provide both a rationale and intuition for how to\n",
        "use these designs when designing your own deep convolutional neural network models. \n",
        "\n",
        "In this section, you will discover the key architectural milestones for the use of convolutional neural networks for challenging image classification problems. After completing this tutorial, you will\n",
        "know:\n",
        "\n",
        "- How to pattern the number of filters and filter sizes when implementing convolutional neural networks.\n",
        "\n",
        "- How to arrange convolutional and pooling layers in a uniform pattern to develop well-performing models.\n",
        "\n",
        "- How to use the inception module and residual module to develop much deeper convolutional networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7sW5mjiXK2r"
      },
      "source": [
        "## 2.1 Architectural Design for CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKQe5EHoYpcE"
      },
      "source": [
        "The elements of a convolutional neural network, such as convolutional and pooling layers, are relatively straightforward to understand. The challenging part of using convolutional neural networks in practice is how to design model architectures that best use these simple elements. A helpful approach to learning how to design effective convolutional neural network architectures is to study successful applications. This is remarkably straightforward to do because of the intense study and application of CNNs from 2012 to 2016 for the ImageNet Large Scale Visual Recognition Challenge, or ILSVRC. This challenge resulted in both the rapid advancement in the state-of-the-art for challenging computer vision tasks and the development of general innovations in the architecture of convolutional neural network models.\n",
        "\n",
        "We will begin with **LeNet-5** that is often described as the first successful and important application of CNNs prior to the ILSVRC, then look at four different winning architectural innovations for CNNs developed for the ILSVRC, namely, **AlexNet**, **VGG**, **Inception**, and **ResNet**. By understanding these milestone models and their architecture or architectural innovations from a high-level, you will develop both an appreciation for the use of these architectural elements in modern applications of CNNs in computer vision, and be able to identify and choose architecture elements that may be useful in the design of your own models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55GzB8QgaBId"
      },
      "source": [
        "## 2.2 LeNet-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc70EZluafYi"
      },
      "source": [
        "Perhaps the first widely known and successful application of convolutional neural networks was **LeNet-5**, described by Yann LeCun, et al. in their 1998 paper titled [Gradient-Based Learning Applied to Document Recognition](https://ieeexplore.ieee.org/document/726791). The system was developed for use in a handwritten character recognition problem and demonstrated on the **MNIST standard dataset**, achieving approximately 99.2% classification accuracy (or a 0.8% error rate). The network was then described as the central technique in a broader system referred to as **Graph Transformer Networks**.\n",
        "\n",
        "It is a long paper, and perhaps the best part to focus on is Section II. B. that describes the LeNet-5 architecture. In that section, the paper describes the **network as having seven layers** with input **grayscale images** having the shape **32 x 32**, the size of images in the **MNIST dataset**. \n",
        "\n",
        "> The model proposes a pattern of a convolutional layer followed by an average pooling layer, referred to as a **subsampling layer**. \n",
        "\n",
        "This pattern is repeated two and a half times before the output feature maps are flattened and fed to some fully connected layers for interpretation and a final prediction. A picture of the network architecture is provided in the paper and reproduced below.\n",
        "\n",
        "<img width=\"800\" src=\"https://drive.google.com/uc?export=view&id=1nqbLzHfqorX80I8upHMWINwPNfrmLW-V\"/>\n",
        "\n",
        "The pattern of blocks of convolutional layers and pooling layers (referred to as **subsampling**) grouped and repeated **remains a typical pattern in designing and using convolutional neural networks today, more than twenty years later**. Interestingly, the architecture uses a small number of filters with a modest size as the first hidden layer, specifically 6 filters, each with 5x5 pixels. After pooling, another convolutional layer has many more filters, again with the same size, precisely 16 filters with 5x5 pixels, again followed by pooling. In the repetition of these two blocks of convolution and pooling layers, the trend increases the number of filters.\n",
        "\n",
        "Compared to modern applications, the number of filters is also small, but **the trend of increasing the number of filters with the depth of the network also remains a common pattern in modern usage of the technique.** The flattening of the feature maps and interpretation and classification of the extracted features by fully connected layers also remains a common pattern today. \n",
        "\n",
        "> In modern terminology, the **final section of the architecture** is often referred to as the **classifier**, whereas the **convolutional and pooling layers** earlier in the model are referred to as the **feature extractor**.\n",
        "\n",
        "We can summarize the key aspects of the architecture relevant in modern models as follows:\n",
        "\n",
        "- Fixed-sized input images.\n",
        "- Group convolutional and pooling layers into blocks.\n",
        "- Repetition of convolutional-pooling blocks in the architecture.\n",
        "- Increase in the number of filters with the depth of the network.\n",
        "- Distinct feature extraction and classifier parts of the architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi1tAkmhVIYu"
      },
      "source": [
        "### 2.2.1 Implementing the LeNet-5 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBU9-5-TxLfe"
      },
      "source": [
        "%%capture\n",
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEGdKdb9xQ3f"
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVAEJgbHq3Zb"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D,AveragePooling2D,Flatten,Dense\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "import pytz\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIif9rIRhAun"
      },
      "source": [
        "# load the datasets\n",
        "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
        "train_x = train_x / 255.0\n",
        "test_x = test_x / 255.0\n",
        "\n",
        "# train and test sets\n",
        "train_x = tf.expand_dims(train_x, 3)\n",
        "test_x = tf.expand_dims(test_x, 3)\n",
        "\n",
        "# print shape\n",
        "print(\"Train shape: {0:}\".format(train_x.shape))\n",
        "print(\"Test shape: {0:}\".format(test_x.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IECW3g0tka1E"
      },
      "source": [
        "train_x[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPx0ngMJ4drt"
      },
      "source": [
        "<img width=\"800\" src=\"https://drive.google.com/uc?export=view&id=1nqbLzHfqorX80I8upHMWINwPNfrmLW-V\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ4qC9URgfIG"
      },
      "source": [
        "# Set an experiment name to group training and evaluation\n",
        "experiment_name = wandb.util.generate_id()\n",
        "\n",
        "# setup wandb\n",
        "wandb.init(project=\"lesson06\", \n",
        "           group=experiment_name,\n",
        "           config={\n",
        "               \"filter_c1\": 6,\n",
        "               \"filter_c1_size\": (5,5),\n",
        "               \"filter_c3\": 16,\n",
        "               \"filter_c3_size\": (5,5),\n",
        "               \"layer_c5\": 120,\n",
        "               \"layer_f6\": 84,\n",
        "               \"loss\": \"sparse_categorical_crossentropy\",\n",
        "               \"metric\": \"accuracy\",\n",
        "               \"epoch\": 6,\n",
        "               \"batch_size\": 32,\n",
        "               \"optimizer\": \"adam\"\n",
        "           })\n",
        "config = wandb.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-rs3zMOhfTs"
      },
      "source": [
        "# \n",
        "# create LeNet-5 model\n",
        "#\n",
        "# it is composed of the 8 layers (5 layers considering FC as one layer) such as:\n",
        "#      - 2 convolutional layers\n",
        "#      - 2 subsampling (avg pooling) layers\n",
        "#      - 1 flatten layer\n",
        "#      - 2 fully connected layers\n",
        "#      - 1 output layer with 10 outputs\n",
        "\n",
        "lenet5 = Sequential()\n",
        "\n",
        "lenet5.add(Conv2D(config.filter_c1, config.filter_c1_size, strides=1,  activation='tanh', input_shape=(28,28,1), padding='same')) #C1\n",
        "lenet5.add(AveragePooling2D()) #S2\n",
        "lenet5.add(Conv2D(config.filter_c3, config.filter_c3_size, strides=1, activation='tanh', padding='valid')) #C3\n",
        "lenet5.add(AveragePooling2D()) #S4\n",
        "lenet5.add(Flatten()) #Flatten\n",
        "lenet5.add(Dense(config.layer_c5, activation='tanh')) #C5\n",
        "lenet5.add(Dense(config.layer_f6, activation='tanh')) #F6\n",
        "lenet5.add(Dense(10, activation='softmax')) #Output layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyhpMURGhjOl"
      },
      "source": [
        "lenet5.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezyJZXBUmj07"
      },
      "source": [
        "%%wandb\n",
        "\n",
        "# configure the optimizer, loss, and metrics to monitor.\n",
        "lenet5.compile(optimizer=config.optimizer,\n",
        "               loss=config.loss, \n",
        "               metrics=[config.metric])\n",
        "\n",
        "# training \n",
        "history = lenet5.fit(x=train_x,\n",
        "                    y=train_y,\n",
        "                    batch_size=config.batch_size,\n",
        "                    epochs=config.epoch,\n",
        "                    validation_data=(test_x,test_y),\n",
        "                    callbacks=[WandbCallback()])\n",
        "\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYvs8Rx2Bf7T"
      },
      "source": [
        "**Log Analysis**\n",
        "\n",
        "Next, log an analysis run, using the same experiment name as the group parameter so that this run and the previous run are grouped together in W&B."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TklsWV-FBcSB"
      },
      "source": [
        "%%capture\n",
        "# Install dependencies\n",
        "!pip install scikit-plot -qqq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U3M91iyBsZl"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from scikitplot.metrics import plot_confusion_matrix, plot_roc, plot_precision_recall\n",
        "\n",
        "wandb.init(project=\"lesson06\", group=experiment_name)\n",
        "\n",
        "# Class proportions\n",
        "labels = [str(i) for i in list(set(test_y))]\n",
        "wandb.log({'Class Proportions': wandb.sklearn.plot_class_proportions(train_y,test_y,labels)}, commit=False) # Hold on, more incoming!\n",
        "\n",
        "# Log F1 Score\n",
        "test_y_pred = np.asarray(lenet5.predict(test_x))\n",
        "test_y_pred_class = np.argmax(test_y_pred, axis=1)\n",
        "f1 = f1_score(test_y, test_y_pred_class, average='micro')\n",
        "wandb.log({\"f1\": f1}, commit=False)\n",
        "\n",
        "# Log Confusion Matrix\n",
        "fig, ax = plt.subplots(figsize=(16, 12))\n",
        "plot_confusion_matrix(test_y, test_y_pred_class, ax=ax)\n",
        "wandb.log({\"confusion_matrix\": wandb.Image(fig)}, commit=False)\n",
        "\n",
        "# Log ROC Curve\n",
        "fig, ax = plt.subplots(figsize=(16, 12))\n",
        "plot_roc(test_y, test_y_pred, ax=ax)\n",
        "wandb.log({\"plot_roc\": wandb.Image(fig)},commit=False)  # Now we've logged everything for this step\n",
        "\n",
        "# Precision vs Recall\n",
        "fig, ax = plt.subplots(figsize=(16, 12))\n",
        "plot_precision_recall(test_y, test_y_pred, ax=ax)\n",
        "wandb.log({\"plot_precision_recall\": wandb.Image(fig)},commit=False)  # Now we've logged everything for this step\n",
        "\n",
        "# Class Scores\n",
        "class_score_data = []\n",
        "for test, pred in zip(test_y, test_y_pred):\n",
        "    class_score_data.append([test, pred])\n",
        "\n",
        "wandb.log({\"class_scores\": wandb.Table(data=class_score_data,\n",
        "                                           columns=[\"test\", \"pred\"])}, commit=False)\n",
        "\n",
        "# \n",
        "# Visualize Predictions\n",
        "# \n",
        "# visualize 18 numbers\n",
        "def show_image(train_image, label, index):\n",
        "    plt.subplot(3, 6, index+1)\n",
        "    plt.imshow(tf.squeeze(train_image), cmap=plt.cm.gray)\n",
        "    plt.title(label)\n",
        "    plt.grid(b=False)\n",
        "\n",
        "# predictions\n",
        "predictions = lenet5.predict(test_x)\n",
        "results = np.argmax(predictions, axis = 1)\n",
        "\n",
        "# visualize the first 18 test results\n",
        "plt.figure(figsize=(12, 8))\n",
        "for index in range(18):\n",
        "    label = results[index]\n",
        "    image_pixels = test_x[index,:,:,:]\n",
        "    show_image(image_pixels, label, index)\n",
        "plt.tight_layout()\n",
        "\n",
        "wandb.log({\"Predictions\": plt}, commit=True)\n",
        "\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M44zgGg90A4"
      },
      "source": [
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = lenet5.predict(test_x, batch_size=32)\n",
        "print(classification_report(test_y,predictions.argmax(axis=1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElCu4byeuZjo"
      },
      "source": [
        "### 2.2.2 Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blctuqAA-5G5"
      },
      "source": [
        "According to [Goodfellow et al.](https://www.deeplearningbook.org/), regularization is\n",
        "\n",
        "> “(...) any modification we make to a learning algorithm that is intended to reduce its generalization error, but not its training error”\n",
        "\n",
        "In short, regularization seeks to reduce our testing error perhaps at the expense of increasing training error slightly.\n",
        "\n",
        "We’ve already looked at different forms of regularization in the first part of this course; however, these were parameterized forms of regularization, requiring us to update our loss/update\n",
        "function. In fact, there exist other types of regularization that either:\n",
        "\n",
        "1. Modify the network architecture itself.\n",
        "2. Augment the data passed into the network for training.\n",
        "\n",
        "**Dropout** is a great example of modifying a network architecture by achieving greater generalizability. Here we insert a layer that randomly disconnects nodes from the previous layer to the next layer, thereby ensuring that no single node is responsible for learning how to represent a given class.\n",
        "\n",
        "In this section we’ll be discussing another type of regularization called **data augmentation**. This method purposely perturbs training examples, changing their appearance slightly, before passing them into the network for training. The end result is that a network consistently sees “new” training data points generated from the original training data, partially alleviating the need for us to gather more training data (though in general, gathering more training data will rarely hurt your algorithm).\n",
        "\n",
        "**Data augmentation** encompasses a wide range of techniques used to generate new training samples from the original ones by applying random jitters and perturbations such that the classes labels are\n",
        "not changed. \n",
        "\n",
        "> Our goal when applying **data augmentation** is to increase the generalizability of the model. \n",
        "\n",
        "Given that our network is constantly seeing new, slightly modified versions of the input data points, it’s able to learn more robust features. \n",
        "\n",
        "> At testing time, we do not apply data augmentation\n",
        "and evaluate our trained network – in most cases, you’ll see an increase in testing accuracy, perhaps at the expense at a slight dip in training accuracy.\n",
        "\n",
        "<center><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1PWNBYi_ziF8YnCCd25vnmsf9nxq-KsGH\"></center><center><b>Left</b>: A sample of 250 data points that follow a normal distribution exactly. <b>Right</b>: Adding a small amount of random “jitter” to the distribution. This type of data augmentation can\n",
        "increase the generalizability of our networks.</center>\n",
        "\n",
        "\n",
        "Let’s consider the Figure above (**left**) of a normal distribution with zero mean and unit variance. Training a machine learning model on this data may result in us modeling the distribution exactly –\n",
        "however, in real-world applications, data rarely follows such a neat distribution.\n",
        "\n",
        "Instead, to increase the generalizability of our classifier, we may first randomly jitter points along the distribution by adding some values e drawn from a random distribution (**right**). Our plot\n",
        "still follows an **approximately normal distribution**, but it’s not a perfect distribution as on the left. A model trained on this data is more likely to generalize to example data points not included in the\n",
        "training set.\n",
        "  **In the context of computer vision, data augmentation lends itself naturally**. For example, we can obtain additional training data from the original images by apply simple geometric transforms such as random:\n",
        "\n",
        "1. Translations\n",
        "2. Rotations\n",
        "3. Changes in scale\n",
        "4. Shearing\n",
        "5. Horizontal (and in some cases, vertical) flips\n",
        "\n",
        "Applying a (small) amount of these transformations to an input image will change its appearance slightly, but it does not change the class label – thereby making data augmentation a very natural, easy method to apply to deep learning for computer vision tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DBD44X1ujyO"
      },
      "source": [
        "# load the datasets\n",
        "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
        "train_x = train_x / 255.0\n",
        "test_x = test_x / 255.0\n",
        "\n",
        "# train and test sets\n",
        "train_x = tf.expand_dims(train_x, 3)\n",
        "test_x = tf.expand_dims(test_x, 3)\n",
        "\n",
        "# print shape\n",
        "print(\"Train shape: {0:}\".format(train_x.shape))\n",
        "print(\"Test shape: {0:}\".format(test_x.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cpa4fRxmuweP"
      },
      "source": [
        "# visualize 18 numbers\n",
        "def show_image(train_image, label, index):\n",
        "    plt.subplot(3, 6, index+1)\n",
        "    plt.imshow(tf.squeeze(train_image), cmap=plt.cm.gray)\n",
        "    plt.title(label)\n",
        "    plt.grid(b=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m43Dq3WowsCk"
      },
      "source": [
        "# visualize the first 18 numbers\n",
        "plt.figure(figsize=(12, 8))\n",
        "for index in range(18):\n",
        "    label = train_y[index]\n",
        "    image_pixels = train_x[index,:,:,:]\n",
        "    show_image(image_pixels, label, index)\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWaYGtKGx4RO"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# construct the image generator for data augmentation then\n",
        "# initialize the total number of images generated thus far\n",
        "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
        "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
        "                         horizontal_flip=False, fill_mode=\"nearest\")\n",
        "total = 0\n",
        "image = train_x[10:11,:,:,:]\n",
        "\n",
        "# construct the actual Python generator\n",
        "print(\"[INFO] generating images...\")\n",
        "imageGen = aug.flow(image, batch_size=1)\n",
        "\n",
        "# create a figure\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# loop over examples from our image data augmentation generator\n",
        "for img in imageGen:\n",
        "\n",
        "  show_image(img, train_y[10], total)\n",
        "\n",
        "  # increment our counter\n",
        "  total += 1\n",
        "\n",
        "  # if we have reached 10 examples, break from the loop\n",
        "  if total == 18:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPUQvOYoDwPW"
      },
      "source": [
        "%%capture\n",
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igjMhzxjDwPX"
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5Jxk-q_DwPX"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D,AveragePooling2D,Flatten,Dense\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "import pytz\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh-WSY-SD7MF"
      },
      "source": [
        "# Set an experiment name to group training and evaluation\n",
        "experiment_name = wandb.util.generate_id()\n",
        "\n",
        "# setup wandb\n",
        "wandb.init(project=\"lesson06\", \n",
        "           group=experiment_name,\n",
        "           config={\n",
        "               \"filter_c1\": 6,\n",
        "               \"filter_c1_size\": (5,5),\n",
        "               \"filter_c3\": 16,\n",
        "               \"filter_c3_size\": (5,5),\n",
        "               \"layer_c5\": 120,\n",
        "               \"layer_f6\": 84,\n",
        "               \"loss\": \"sparse_categorical_crossentropy\",\n",
        "               \"metric\": \"accuracy\",\n",
        "               \"epoch\": 6,\n",
        "               \"batch_size\": 32,\n",
        "               \"optimizer\": \"adam\"\n",
        "           })\n",
        "config = wandb.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZHefRSpEFqy"
      },
      "source": [
        "# \n",
        "# create LeNet-5 model\n",
        "#\n",
        "# it is composed of the 8 layers such as:\n",
        "#      - 2 convolutional layers\n",
        "#      - 2 subsampling (avg pooling) layers\n",
        "#      - 1 flatten layer\n",
        "#      - 2 fully connected layers\n",
        "#      - 1 output layer with 10 outputs\n",
        "\n",
        "lenet5 = Sequential()\n",
        "\n",
        "lenet5.add(Conv2D(config.filter_c1, config.filter_c1_size, strides=1,  activation='tanh', input_shape=(28,28,1), padding='same')) #C1\n",
        "lenet5.add(AveragePooling2D()) #S2\n",
        "lenet5.add(Conv2D(config.filter_c3, config.filter_c3_size, strides=1, activation='tanh', padding='valid')) #C3\n",
        "lenet5.add(AveragePooling2D()) #S4\n",
        "lenet5.add(Flatten()) #Flatten\n",
        "lenet5.add(Dense(config.layer_c5, activation='tanh')) #C5\n",
        "lenet5.add(Dense(config.layer_f6, activation='tanh')) #F6\n",
        "lenet5.add(Dense(10, activation='softmax')) #Output layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZWhskvvEFqz"
      },
      "source": [
        "lenet5.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZqnqNq5EFqz"
      },
      "source": [
        "%%wandb\n",
        "\n",
        "# configure the optimizer, loss, and metrics to monitor.\n",
        "lenet5.compile(optimizer=config.optimizer,\n",
        "               loss=config.loss, \n",
        "               metrics=[config.metric])\n",
        "\n",
        "# construct the image generator for data augmentation then\n",
        "# initialize the total number of images generated thus far\n",
        "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
        "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
        "                         horizontal_flip=False, fill_mode=\"nearest\")\n",
        "\n",
        "print(\"[INFO] training network...\")\n",
        "history = lenet5.fit(aug.flow(train_x, train_y, batch_size=config.batch_size),\n",
        "                     validation_data=(test_x, test_y), \n",
        "                     epochs=config.epoch, \n",
        "                      callbacks=[WandbCallback()])\n",
        "\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXQwq5LhFfjr"
      },
      "source": [
        "%%capture\n",
        "# Install dependencies\n",
        "!pip install scikit-plot -qqq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f37KaOGjFfjt"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from scikitplot.metrics import plot_confusion_matrix, plot_roc, plot_precision_recall\n",
        "\n",
        "wandb.init(project=\"lesson06\", group=experiment_name)\n",
        "\n",
        "# Class proportions\n",
        "labels = [str(i) for i in list(set(test_y))]\n",
        "wandb.log({'Class Proportions': wandb.sklearn.plot_class_proportions(train_y,test_y,labels)}, commit=False) # Hold on, more incoming!\n",
        "\n",
        "# Log F1 Score\n",
        "test_y_pred = np.asarray(lenet5.predict(test_x))\n",
        "test_y_pred_class = np.argmax(test_y_pred, axis=1)\n",
        "f1 = f1_score(test_y, test_y_pred_class, average='micro')\n",
        "wandb.log({\"f1\": f1}, commit=False)\n",
        "\n",
        "# Log Confusion Matrix\n",
        "fig, ax = plt.subplots(figsize=(16, 12))\n",
        "plot_confusion_matrix(test_y, test_y_pred_class, ax=ax)\n",
        "wandb.log({\"confusion_matrix\": wandb.Image(fig)}, commit=False)\n",
        "\n",
        "# Log ROC Curve\n",
        "fig, ax = plt.subplots(figsize=(16, 12))\n",
        "plot_roc(test_y, test_y_pred, ax=ax)\n",
        "wandb.log({\"plot_roc\": wandb.Image(fig)},commit=False)  # Now we've logged everything for this step\n",
        "\n",
        "# Precision vs Recall\n",
        "fig, ax = plt.subplots(figsize=(16, 12))\n",
        "plot_precision_recall(test_y, test_y_pred, ax=ax)\n",
        "wandb.log({\"plot_precision_recall\": wandb.Image(fig)},commit=False)  # Now we've logged everything for this step\n",
        "\n",
        "# Class Scores\n",
        "class_score_data = []\n",
        "for test, pred in zip(test_y, test_y_pred):\n",
        "    class_score_data.append([test, pred])\n",
        "\n",
        "wandb.log({\"class_scores\": wandb.Table(data=class_score_data,\n",
        "                                           columns=[\"test\", \"pred\"])}, commit=False)\n",
        "\n",
        "# \n",
        "# Visualize Predictions\n",
        "# \n",
        "# visualize 18 numbers\n",
        "def show_image(train_image, label, index):\n",
        "    plt.subplot(3, 6, index+1)\n",
        "    plt.imshow(tf.squeeze(train_image), cmap=plt.cm.gray)\n",
        "    plt.title(label)\n",
        "    plt.grid(b=False)\n",
        "\n",
        "# predictions\n",
        "predictions = lenet5.predict(test_x)\n",
        "results = np.argmax(predictions, axis = 1)\n",
        "\n",
        "# visualize the first 18 test results\n",
        "plt.figure(figsize=(12, 8))\n",
        "for index in range(18):\n",
        "    label = results[index]\n",
        "    image_pixels = test_x[index,:,:,:]\n",
        "    show_image(image_pixels, label, index)\n",
        "plt.tight_layout()\n",
        "\n",
        "wandb.log({\"Predictions\": plt}, commit=True)\n",
        "\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5jZT8cO5Oiu"
      },
      "source": [
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = lenet5.predict(test_x, batch_size=32)\n",
        "print(classification_report(test_y,predictions.argmax(axis=1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3zfoqpWeXUd"
      },
      "source": [
        "### 2.2.3 Extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kVRLuFxeZkx"
      },
      "source": [
        "This section lists some ideas for extending that you may wish to explore.\n",
        "\n",
        "- **Batch normalization**. Implement BN technique before and after the activation function and\n",
        "review the final result.\n",
        "- **Other activation functions**. Investigate change the activation function to relu and compare the results.\n",
        "\n",
        "If you explore any of these extensions, I’d love to know."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdZ_Y3GYK2rX"
      },
      "source": [
        "## 2.3 AlexNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F81wxQ7lK6I7"
      },
      "source": [
        "The work that perhaps could be credited with sparking renewed interest in neural networks and the beginning of the dominance of deep learning in many computer vision applications was the 2012 paper by Alex Krizhevsky et al. titled [ImageNet Classification with Deep Convolutional\n",
        "Neural Networks](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf). The paper describes a model later referred to as **AlexNet** designed to address the ImageNet Large Scale Visual Recognition Challenge or ILSVRC-2010 competition for classifying photographs of objects into one of 1,000 different categories.\n",
        "\n",
        "The ILSVRC was a competition designed to spur innovation in the field of computer vision. Before the development of AlexNet, the task was thought very difficult and far beyond the capability of modern computer vision methods. \n",
        "\n",
        "> AlexNet successfully demonstrated the capability\n",
        "of the convolutional neural network model in the domain and kindled a fire that resulted in many more improvements and innovations, many demonstrated on the same ILSVRC task in subsequent years. \n",
        "\n",
        "More broadly, **the paper showed that it is possible to develop deep and effective end-to-end models** for a challenging problem without using unsupervised pre-training techniques popular at the time.\n",
        "\n",
        "Important in the design of AlexNet was a suite of new or successful methods, but not widely adopted at the time. Now, they have become requirements when using CNNs for image classification. \n",
        "\n",
        "> AlexNet used the rectified linear activation function, or ReLU, as the nonlinearly after each convolutional layer, instead of S-shaped functions such as the logistic or Tanh that were common up until that point. A softmax activation function was used in the output layer, now a staple for multiclass classification with neural networks.\n",
        "\n",
        "The average pooling used in LeNet-5 was replaced with a max-pooling method, although in this case, overlapping pooling was found to outperform non-overlapping pooling that is commonly used today (e.g., stride of pooling operation is the same size as the pooling operation,\n",
        "e.g., 2 by 2 pixels). The newly proposed dropout method was used to address overfitting between the fully connected layers of the classifier part of the model to improve generalization error. The architecture of AlexNet is deep and extends upon some of the patterns established\n",
        "with LeNet-5. The image below, taken from the paper, summarizes the model architecture, in this case, split into two pipelines to train on the GPU hardware of the time.\n",
        "\n",
        "<img width=\"800\" src=\"https://drive.google.com/uc?export=view&id=111DrLxQn-ejJ2-7zNA4M8t78wmqcMQNe\"/>\n",
        "\n",
        "\n",
        "**It is similar to LeNet-5, only much larger and deeper**, and it was the first to stack convolutional layers directly on top of one another, instead of stacking a pooling layer on top of each convolutional layer. Table below presents this architecture.\n",
        "\n",
        "<center><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=193aOD83q_m_apxqjv1kFSjGRYFV7HfL2\"></center><center>AlexNet Architecture.</center>\n",
        "\n",
        "\n",
        "The model has **five convolutional layers** in the **feature extraction part** of the model and **three fully connected layers** in the **classifier part** of the model. Input images were fixed to the size **227x227 (there is a typo in the original paper using 224 x 224) with three color channels**. In terms of the number of **filters** used in each convolutional layer, the pattern of increasing the number of filters with depth seen in LeNet was mainly adhered to; in this case, the sizes: **96, 256, 384, 384, and 256**. Similarly, the **pattern of decreasing the size of the filter** (kernel) with depth was used, starting from the smaller size of 11x11 and decreasing to 5x5, and then to 3x3 in the deeper layers. **The use of small filters such as 5x5 and 3x3 is now the norm**.\n",
        "\n",
        "The pattern of a convolutional layer followed by a pooling layer was used at the start and end of the feature detection part of the model. Interestingly, **a pattern of a convolutional layer followed immediately by a second convolutional** layer was used. **This pattern too has become a modern standard**. \n",
        "\n",
        "To reduce overfitting, the authors used **two regularization techniques**. First, they applied dropout with a 50% dropout rate during training to the outputs of layers F9 and F10. Second, they performed **data augmentation** by randomly shifting the training images by various offsets, flipping them horizontally, and changing the lighting conditions.\n",
        "\n",
        "**Data augmentation** artificially increases the size of the training set by generating many realistic variants of each training instance. **This reduces overfitting**, making this a regularization technique. The generated instances should be as realistic as possible: ideally, given an image from the augmented training set, a human should not be able to tell whether it was augmented or not. Simply adding white noise will not help; the modifications should be learnable (white noise is not).\n",
        "\n",
        "AlexNet also uses a competitive normalization step immediately after the ReLU step of layers C1 and C3, called **Local Response Normalization (LRN)**: the most strongly activated neurons inhibit other neurons located at the same position in neighboring feature maps (such competitive activation has been observed in biological neurons). This encourages different feature maps to specialize, pushing them apart and forcing them to explore a wider range of features, ultimately improving generalization.\n",
        "\n",
        "\n",
        "\n",
        "We can summarize the key aspects of the architecture relevant in modern models as follows:\n",
        "\n",
        "- Use of the ReLU activation function after convolutional layers and softmax for the output layer.\n",
        "- Use of Max Pooling instead of Average Pooling.\n",
        "- Use of Dropout regularization between the fully connected layers.\n",
        "- The pattern of a convolutional layer fed directly to another convolutional layer.\n",
        "- Use of Data Augmentation.\n",
        "\n",
        "\n",
        "> A variant of AlexNet called [ZF Net](https://arxiv.org/abs/1311.2901) was developed by Matthew Zeiler and Rob Fergus and won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked hyperparameters (number of feature maps, kernel size, stride, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3MfpRAUMGq6"
      },
      "source": [
        "### 2.3.1 Implementing the AlexNet model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHGqW9dCshPQ"
      },
      "source": [
        "For the sake of understanding, look the table above and compare the hyperparameters and dimensions of each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAn06umgc-2I"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# create a model\n",
        "model = Sequential()\n",
        "\n",
        "# \n",
        "# Block #1: first CONV => RELU => POOL layer set\n",
        "#\n",
        "model.add(Conv2D(96, (11, 11), strides=(4, 4),\n",
        "                 input_shape=(227,227,3), padding=\"valid\",\n",
        "                 kernel_regularizer=l2(0.0002),activation='relu'))\n",
        "\n",
        "# Batch Normalization does not exist in 2012, here is a modification of original proposal\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# \n",
        "# Block #2: second CONV => RELU => POOL layer set\n",
        "#\n",
        "model.add(Conv2D(256, (5, 5), padding=\"same\",\n",
        "                 kernel_regularizer=l2(0.0002),activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# \n",
        "# Block #3: CONV => RELU => CONV => RELU => CONV => RELU\n",
        "#\n",
        "model.add(Conv2D(384, (3, 3), padding=\"same\",\n",
        "                 kernel_regularizer=l2(0.0002),activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(384, (3, 3), padding=\"same\",\n",
        "                 kernel_regularizer=l2(0.002),activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (3, 3), padding=\"same\",\n",
        "                 kernel_regularizer=l2(0.002),activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# \n",
        "# Block #4: first set of FC => RELU layers\n",
        "#\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4096, kernel_regularizer=l2(0.0002),activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# \n",
        "# Block #5: second set of FC => RELU layers\n",
        "#\n",
        "model.add(Dense(4096, kernel_regularizer=l2(0.0002),activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# \n",
        "# softmax classifier\n",
        "#\n",
        "model.add(Dense(1000, kernel_regularizer=l2(0.0002)))\n",
        "model.add(Activation(\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfVqZPnedAsC"
      },
      "source": [
        "# 62M parameters!!!\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4InqRNPVtcmj"
      },
      "source": [
        "# 3 Working with HDFS files and large datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVjlaBJEn_db"
      },
      "source": [
        "So far in this course, we have only worked with datasets that can fit into the main memory of our machines (colab). For small datasets, this is a reasonable assumption – we load each individual image, preprocess it, and allow it to be fed through our network. However, for large scale deep\n",
        "learning datasets (e.g., ImageNet), we need to create data generators that access only a portion of the dataset at a time (i.e., a mini-batch), then allow the batch to be passed through the network.\n",
        "\n",
        "Luckily, Keras ships with methods that allow you to use the raw file paths on disk as inputs to a training process. You do not have to store the entire dataset in memory – simply supply the image paths to the Keras data generator and your images will be loaded in batches and fed through the\n",
        "network. However, this method is terribly inefficient. Each and every image residing on your disk requires an I/O operation which introduces latency into your training pipeline. Training deep learning networks is already slow enough – we would do well to avoid the I/O bottleneck as much as possible.\n",
        "\n",
        "**A more elegant solution would be to generate an HDF5 dataset for your raw images**. Not only is HDF5 capable of storing massive datasets, but it’s optimized for I/O operations, **especially for extracting batches (called “slices”)** from the file. As we’ll see throughout the remainder of this course, taking the extra step to pack the raw images residing on disk into an HDF5 file allows us to construct a deep learning framework that can be used to rapidly build datasets and train deep learning networks on top of them.\n",
        "\n",
        "In the remainder of this section, we’ll demonstrate how to construct an HDF5 dataset for the [Kaggle Dogs vs. Cats competition](https://www.kaggle.com/c/dogs-vs-cats). Right after,  we’ll use this HDF5 dataset to train the seminal [AlexNet architecture](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf), eventually resulting in a top-25 position on the leaderboard in the respective competition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAB44gZbfYAq"
      },
      "source": [
        "## 3.1 What is HDF5?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY9ZvSgVfdxF"
      },
      "source": [
        "**HDF5** is binary data format created by the [HDF5 group](https://www.hdfgroup.org/solutions/hdf5/) to store gigantic numerical datasets on disk (far too large to store in memory) while facilitating easy access and computation on the rows of the datasets. \n",
        "\n",
        "> Data in HDF5 is stored hierarchically, similar to how a file system stores data. \n",
        "\n",
        "Data is first defined in groups, where a group is a container-like structure which can hold datasets and other\n",
        "groups. Once a group has been defined, a dataset can be created within the group. A dataset can be\n",
        "thought of as a multi-dimensional array (i.e., a NumPy array) of a homogeneous data type (integer,\n",
        "float, unicode, etc.). An example of an HDF5 file containing a group with multiple datasets is\n",
        "displayed in Figure below.\n",
        "\n",
        "<center><img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1-oiHlD5B97FTA3p9pnnm5PJhS4ywI_f_\"></center><center>An example of a HDF5 file with three datasets. The first dataset contains the\n",
        "label_names for CALTECH-101. We then have labels, which maps the each image to its\n",
        "corresponding class label. Finally, the features dataset contains the image quantifications extracted\n",
        "by the CNN</center>\n",
        "\n",
        "**HDF5 is written in C**; however, by using the [h5py module](h5py.org), we can gain access to\n",
        "the underlying C API using the Python programming language. What makes **h5py** so awesome\n",
        "is the **ease of interaction with data**. \n",
        "\n",
        "> We can store huge amounts of data in our HDF5 dataset and manipulate the data in a NumPy-like fashion. \n",
        "\n",
        "For example, we can use standard Python syntax to access and slice rows from multi-terabyte datasets stored on disk as if they were simple NumPy arrays loaded into memory. Thanks to specialized data structures, these slices and row accesses are lighting quick.\n",
        "\n",
        "When using HDF5 with h5py, **you can think of your data as a gigantic NumPy array**\n",
        "that is too large to fit into main memory but can still be accessed and manipulated just the same.\n",
        "Perhaps best of all, **the HDF5 format is standardized**\n",
        "\n",
        "> meaning that datasets stored in HDF5 format are inherently portable and can be accessed by other developers using different programming languages such as C, MATLAB, and Java.\n",
        "\n",
        "We’ll be writing a custom Python class that allows us to efficiently accept input data and write it to an HDF5 dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zisp6OplmCSt"
      },
      "source": [
        "## 3.2 Writing to an HDF5 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F96t7jk8mOXZ"
      },
      "source": [
        "Before we can even think about treating CNN Architectures, we first need to develop a bit of infrastructure. In particular, we need to define a Python class named **HDF5DatasetWriter**, which as the name suggests, is responsible for taking an input set of NumPy arrays (whether features, raw images, etc.) and writing them to HDF5 format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrT62Vkhv2NZ"
      },
      "source": [
        "# import the necessary packages\n",
        "import h5py\n",
        "import os\n",
        "\n",
        "class HDF5DatasetWriter:\n",
        "  def __init__(self, dims, outputPath, dataKey=\"images\",bufSize=1000):\n",
        "    \"\"\"\n",
        "    The constructor to HDF5DatasetWriter accepts four parameters, two of which are optional.\n",
        "    \n",
        "    Args:\n",
        "    dims: controls the dimension or shape of the data we will be storing in the dataset.\n",
        "    if we were storing the (flattened) raw pixel intensities of the 28x28 = 784 MNIST dataset, \n",
        "    then dims=(70000, 784).\n",
        "    outputPath: path to where our output HDF5 file will be stored on disk.\n",
        "    datakey: The optional dataKey is the name of the dataset that will store\n",
        "    the data our algorithm will learn from.\n",
        "    bufSize: controls the size of our in-memory buffer, which we default to 1,000 feature\n",
        "    vectors/images. Once we reach bufSize, we’ll flush the buffer to the HDF5 dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # check to see if the output path exists, and if so, raise\n",
        "    # an exception\n",
        "    if os.path.exists(outputPath):\n",
        "      raise ValueError(\"The supplied `outputPath` already \"\n",
        "        \"exists and cannot be overwritten. Manually delete \"\n",
        "        \"the file before continuing.\", outputPath)\n",
        "\n",
        "    # open the HDF5 database for writing and create two datasets:\n",
        "    # one to store the images/features and another to store the\n",
        "    # class labels\n",
        "    self.db = h5py.File(outputPath, \"w\")\n",
        "    # \n",
        "    # for resource limitations due to hard-disk space, a compression algorithm can be used, the price is the demand of computational power\n",
        "    #\n",
        "    self.data = self.db.create_dataset(dataKey, dims,dtype=\"float\",compression='gzip')\n",
        "    self.labels = self.db.create_dataset(\"labels\", (dims[0],),dtype=\"int\")\n",
        "\n",
        "    # store the buffer size, then initialize the buffer itself\n",
        "    # along with the index into the datasets\n",
        "    self.bufSize = bufSize\n",
        "    self.buffer = {\"data\": [], \"labels\": []}\n",
        "    self.idx = 0\n",
        "\n",
        "  def add(self, rows, labels):\n",
        "    # add the rows and labels to the buffer\n",
        "    self.buffer[\"data\"].extend(rows)\n",
        "    self.buffer[\"labels\"].extend(labels)\n",
        "\n",
        "    # check to see if the buffer needs to be flushed to disk\n",
        "    if len(self.buffer[\"data\"]) >= self.bufSize:\n",
        "      self.flush()\n",
        "\n",
        "  def flush(self):\n",
        "    # write the buffers to disk then reset the buffer\n",
        "    i = self.idx + len(self.buffer[\"data\"])\n",
        "    self.data[self.idx:i] = self.buffer[\"data\"]\n",
        "    self.labels[self.idx:i] = self.buffer[\"labels\"]\n",
        "    self.idx = i\n",
        "    self.buffer = {\"data\": [], \"labels\": []}\n",
        "\n",
        "  def storeClassLabels(self, classLabels):\n",
        "    # create a dataset to store the actual class label names,\n",
        "    # then store the class labels\n",
        "    dt = h5py.special_dtype(vlen=str) # `vlen=unicode` for Py2.7\n",
        "    labelSet = self.db.create_dataset(\"label_names\",(len(classLabels),), dtype=dt)\n",
        "    labelSet[:] = classLabels\n",
        "\n",
        "  def close(self):\n",
        "    # check to see if there are any other entries in the buffer\n",
        "    # that need to be flushed to disk\n",
        "    if len(self.buffer\n",
        "           [\"data\"]) > 0:\n",
        "      self.flush()\n",
        "\n",
        "    # close the dataset\n",
        "    self.db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3uegkGTcPdB"
      },
      "source": [
        "As you can see, the **HDF5DatasetWriter** doesn’t have much to do with machine learning or deep learning at all – it’s simply a class used to help us store data in HDF5 format. As you continue in your deep learning learning, you’ll notice that much of the initial labor when setting up a new problem is getting the data into a format you can work with. Once you have your data in a format that’s straightforward to manipulate, it becomes substantially easier to apply machine learning and deep learning techniques to your data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmiFiQopnMjn"
      },
      "source": [
        "## 3.3 Downloading Kaggle: Dogs vs. Cats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jQl5CP-pSPF"
      },
      "source": [
        "To download the Kaggle: Dogs vs. Cats dataset you’ll first need to create an account on kaggle.com. From there, head to the [Dogs vs. Cats homepage](https://www.kaggle.com/c/dogs-vs-cats/data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esFTx_wHmni3"
      },
      "source": [
        "# download cat & dogs dataset (catdogs.zip)\n",
        "!gdown https://drive.google.com/uc?id=1RqVl-1AOFRYaktJuG3xxxmuur0ua8s-p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-igquSvNn4G6"
      },
      "source": [
        "# train/dog.NNNN.jpg\n",
        "# train/cat.NNNN.jpg\n",
        "!unzip catdogs.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCmYyH7NoDcS"
      },
      "source": [
        "# we will be using the following data structure for this challenge\n",
        "# \n",
        "# | ----- catdogs\n",
        "# |       | ----- hdf5\n",
        "# |       | ----- output\n",
        "# |       | ----- train\n",
        "\n",
        "!mkdir catdogs\n",
        "!mkdir catdogs/hdf5\n",
        "!mkdir catdogs/output\n",
        "!mv train catdogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQjEjj79tCV5"
      },
      "source": [
        "# 25k instances\n",
        "!ls catdogs/train | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1ia_8Wtr2rB"
      },
      "source": [
        "### 3.3.1 Building the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgP-qM_SrEPN"
      },
      "source": [
        "# define the paths to the images directory\n",
        "IMAGES_PATH = \"catdogs/train\"\n",
        "\n",
        "# since we do not have validation data or access to the testing\n",
        "# labels we need to take a number of images from the training\n",
        "# data and use them instead\n",
        "NUM_CLASSES = 2\n",
        "NUM_VAL_IMAGES = 1250 * NUM_CLASSES\n",
        "NUM_TEST_IMAGES = 1250 * NUM_CLASSES\n",
        "\n",
        "# define the path to the output training, validation, and testing\n",
        "# HDF5 files\n",
        "TRAIN_HDF5 = \"catdogs/hdf5/train.hdf5\"\n",
        "VAL_HDF5 = \"catdogs/hdf5/val.hdf5\"\n",
        "TEST_HDF5 = \"catdogs/hdf5/test.hdf5\"\n",
        "\n",
        "# path to the output model file\n",
        "MODEL_PATH = \"catdogs/alexnet_dogs_vs_cats.model\"\n",
        "\n",
        "# define the path to the dataset mean\n",
        "DATASET_MEAN = \"catdogs/dogs_vs_cats_mean.json\"\n",
        "\n",
        "# define the path to the output directory used for storing plots,\n",
        "# classification reports, etc.\n",
        "OUTPUT_PATH = \"catdogs/output\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atTtOl4gsK_d"
      },
      "source": [
        "> On Line 2 we define the path to the directory containing the dog and cat images – these are the images that we’ll be packing into a HDF5 dataset later in this section.\n",
        "\n",
        "> Lines 7-9 define the total number of class labels (two: one for dog, another for cat) along with the number of validation and testing images (2,500 for each). \n",
        "\n",
        "> We can then specify the path to our output HDF5 files for the training, validation, and testing splits, respectively on Lines 13-15.\n",
        "\n",
        "The second half of the configuration file defines the path to the output serialized weights, the dataset mean, and a general “output” path to store plots, classification reports, logs, etc.\n",
        "\n",
        "The DATASET_MEAN file will be used to store the average red, green, and blue pixel intensity values across the entire (training) dataset. When we train our network, we’ll subtract the mean RGB values from every pixel in the image (the same goes for testing and evaluation as well). This method, called **mean subtraction**, is a type of data normalization technique and is more often used than scaling pixel intensities to the range [0,1] as it’s shown to be more effective on large datasets and deeper neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diFpXCQpv8b4"
      },
      "source": [
        "### 3.3.2 Aspect Aware"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vCPSlWbv4zu"
      },
      "source": [
        "# import the necessary packages\n",
        "import imutils\n",
        "import cv2\n",
        "\n",
        "# useful class to help the resize of images\n",
        "class AspectAwarePreprocessor:\n",
        "\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n",
        "\t\t# store the target image width, height, and interpolation\n",
        "\t\t# method used when resizing\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\t\tself.inter = inter\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# grab the dimensions of the image and then initialize\n",
        "\t\t# the deltas to use when cropping\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\t\tdW = 0\n",
        "\t\tdH = 0\n",
        "\n",
        "\t\t# if the width is smaller than the height, then resize\n",
        "\t\t# along the width (i.e., the smaller dimension) and then\n",
        "\t\t# update the deltas to crop the height to the desired\n",
        "\t\t# dimension\n",
        "\t\tif w < h:\n",
        "\t\t\timage = imutils.resize(image, width=self.width,\n",
        "\t\t\t\tinter=self.inter)\n",
        "\t\t\tdH = int((image.shape[0] - self.height) / 2.0)\n",
        "\n",
        "\t\t# otherwise, the height is smaller than the width so\n",
        "\t\t# resize along the height and then update the deltas\n",
        "\t\t# crop along the width\n",
        "\t\telse:\n",
        "\t\t\timage = imutils.resize(image, height=self.height,\n",
        "\t\t\t\tinter=self.inter)\n",
        "\t\t\tdW = int((image.shape[1] - self.width) / 2.0)\n",
        "\n",
        "\t\t# now that our images have been resized, we need to\n",
        "\t\t# re-grab the width and height, followed by performing\n",
        "\t\t# the crop\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\t\timage = image[dH:h - dH, dW:w - dW]\n",
        "\n",
        "\t\t# finally, resize the image to the provided spatial\n",
        "\t\t# dimensions to ensure our output image is always a fixed\n",
        "\t\t# size\n",
        "\t\treturn cv2.resize(image, (self.width, self.height),\n",
        "\t\t\tinterpolation=self.inter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfC6aT2CwF5r"
      },
      "source": [
        "### 3.3.3 Converter images into HDF5 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvTibrXhtAl6"
      },
      "source": [
        "#\n",
        "# Convert jpg images into hdf5 (train, val, test)\n",
        "#\n",
        "# 25min to 35min\n",
        "\n",
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import progressbar\n",
        "import json\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# grab the paths to the images\n",
        "trainPaths = list(paths.list_images(IMAGES_PATH))\n",
        "trainLabels = [p.split(os.path.sep)[-1].split(\".\")[0] for p in trainPaths]\n",
        "le = LabelEncoder()\n",
        "trainLabels = le.fit_transform(trainLabels)\n",
        "\n",
        "# perform stratified sampling from the training set to build the\n",
        "# testing split from the training data\n",
        "split = train_test_split(trainPaths, trainLabels,\n",
        "                         test_size=NUM_TEST_IMAGES, \n",
        "                         stratify=trainLabels,random_state=42)\n",
        "(trainPaths, testPaths, trainLabels, testLabels) = split\n",
        "\n",
        "# perform another stratified sampling, this time to build the\n",
        "# validation data\n",
        "split = train_test_split(trainPaths, trainLabels,\n",
        "                         test_size=NUM_VAL_IMAGES, \n",
        "                         stratify=trainLabels,random_state=42)\n",
        "(trainPaths, valPaths, trainLabels, valLabels) = split\n",
        "\n",
        "# construct a list pairing the training, validation, and testing\n",
        "# image paths along with their corresponding labels and output HDF5\n",
        "# files\n",
        "datasets = [\n",
        "\t(\"train\", trainPaths, trainLabels, TRAIN_HDF5),\n",
        "\t(\"val\", valPaths, valLabels, VAL_HDF5),\n",
        "\t(\"test\", testPaths, testLabels, TEST_HDF5)]\n",
        "\n",
        "# initialize the image pre-processor and the lists of RGB channel\n",
        "# averages\n",
        "aap = AspectAwarePreprocessor(256, 256)\n",
        "(R, G, B) = ([], [], [])\n",
        "\n",
        "# loop over the dataset tuples\n",
        "for (dType, paths, labels, outputPath) in datasets:\n",
        "\t# create HDF5 writer\n",
        "\tprint(\"[INFO] building {}...\".format(outputPath))\n",
        "\twriter = HDF5DatasetWriter((len(paths), 256, 256, 3), outputPath)\n",
        "\n",
        "\t# initialize the progress bar\n",
        "\twidgets = [\"Building Dataset: \", progressbar.Percentage(), \" \",progressbar.Bar(), \" \", progressbar.ETA()]\n",
        "\tpbar = progressbar.ProgressBar(maxval=len(paths),widgets=widgets).start()\n",
        "\n",
        "\t# loop over the image paths\n",
        "\tfor (i, (path, label)) in enumerate(zip(paths, labels)):\n",
        "\t\t# load the image and process it\n",
        "\t\timage = cv2.imread(path)\n",
        "\t\timage = aap.preprocess(image)\n",
        "\n",
        "\t\t# if we are building the training dataset, then compute the\n",
        "\t\t# mean of each channel in the image, then update the\n",
        "\t\t# respective lists\n",
        "\t\tif dType == \"train\":\n",
        "\t\t\t(b, g, r) = cv2.mean(image)[:3]\n",
        "\t\t\tR.append(r)\n",
        "\t\t\tG.append(g)\n",
        "\t\t\tB.append(b)\n",
        "\n",
        "\t\t# add the image and label # to the HDF5 dataset\n",
        "\t\twriter.add([image], [label])\n",
        "\t\tpbar.update(i)\n",
        "\n",
        "\t# close the HDF5 writer\n",
        "\tpbar.finish()\n",
        "\twriter.close()\n",
        "\n",
        "# construct a dictionary of averages, then serialize the means to a\n",
        "# JSON file\n",
        "print(\"[INFO] serializing means...\")\n",
        "D = {\"R\": np.mean(R), \"G\": np.mean(G), \"B\": np.mean(B)}\n",
        "f = open(DATASET_MEAN, \"w\")\n",
        "f.write(json.dumps(D))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wUg4TL7DQje"
      },
      "source": [
        "# copy hdf5 files for your google drive\n",
        "!cp -r catdogs/hdf5 /content/drive/MyDrive/Atividades/Ensino/Disciplinas/POS-GRADUAÇÃO/Deep\\ Learning/Lessons/Lesson\\ #06/catdogs\n",
        "!cp -r catdogs/dogs_vs_cats_mean.json /content/drive/MyDrive/Atividades/Ensino/Disciplinas/POS-GRADUAÇÃO/Deep\\ Learning/Lessons/Lesson\\ #06/catdogs\n",
        "!cp -r catdogs/output/ /content/drive/MyDrive/Atividades/Ensino/Disciplinas/POS-GRADUAÇÃO/Deep\\ Learning/Lessons/Lesson\\ #06/catdogs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5-gVeJxFYdo"
      },
      "source": [
        "<font color=\"red\"> Only execute the cell below if you already have hdf5 files stored in your google drive </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNS9YzB7EZqx"
      },
      "source": [
        "# \n",
        "# only in case loading data from drive\n",
        "#\n",
        "!mkdir catdogs\n",
        "!cp -r /content/drive/MyDrive/Atividades/Ensino/Disciplinas/POS-GRADUAÇÃO/Deep\\ Learning/Lessons/Lesson\\ #06/catdogs/hdf5 catdogs\n",
        "!cp -r /content/drive/MyDrive/Atividades/Ensino/Disciplinas/POS-GRADUAÇÃO/Deep\\ Learning/Lessons/Lesson\\ #06/catdogs/output catdogs\n",
        "!cp -r /content/drive/MyDrive/Atividades/Ensino/Disciplinas/POS-GRADUAÇÃO/Deep\\ Learning/Lessons/Lesson\\ #06/catdogs/dogs_vs_cats_mean.json catdogs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xOksB6EGKfg"
      },
      "source": [
        "## 3.4 Competing in Kaggle: Dogs vs. Cats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1alnTRfBGN26"
      },
      "source": [
        "### 3.4.1 Image Preprocessors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgmBX1ST1F8U"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "class ImageToArrayPreprocessor:\n",
        "\tdef __init__(self, dataFormat=None):\n",
        "\t\t# store the image data format\n",
        "\t\tself.dataFormat = dataFormat\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# apply the Keras utility function that correctly rearranges\n",
        "\t\t# the dimensions of the image\n",
        "\t\treturn img_to_array(image, data_format=self.dataFormat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XgdXcA7aPfT"
      },
      "source": [
        "### 3.4.2 Mean preprocessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgq3q5uLZj-l"
      },
      "source": [
        "Let’s get started with the mean pre-processor. We will learn how to convert an image\n",
        "dataset to HDF5 format – part of this conversion involved computing the average Red, Green, and Blue pixel intensities across all images in the training dataset. Now that we have these averages, we are going to perform a pixel-wise subtraction of these values from our input images as a **form of data normalization**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfbg-dCsUEuW"
      },
      "source": [
        "# import the necessary packages\n",
        "import cv2\n",
        "\n",
        "class MeanPreprocessor:\n",
        "\tdef __init__(self, rMean, gMean, bMean):\n",
        "\t\t# store the Red, Green, and Blue channel averages across a\n",
        "\t\t# training set\n",
        "\t\tself.rMean = rMean\n",
        "\t\tself.gMean = gMean\n",
        "\t\tself.bMean = bMean\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# split the image into its respective Red, Green, and Blue\n",
        "\t\t# channels\n",
        "\t\t(B, G, R) = cv2.split(image.astype(\"float32\"))\n",
        "\n",
        "\t\t# subtract the means for each channel\n",
        "\t\tR -= self.rMean\n",
        "\t\tG -= self.gMean\n",
        "\t\tB -= self.bMean\n",
        "\n",
        "    # Keep in mind that OpenCV represents images in BGR order\n",
        "\t\t# merge the channels back together and return the image\n",
        "\t\treturn cv2.merge([B, G, R])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COixYybsaWyp"
      },
      "source": [
        "### 3.4.3 Patch preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLxZ6UF7ahNo"
      },
      "source": [
        "The PatchPreprocessor is responsible for randomly sampling MxN regions of an image during the training process. We apply patch preprocessing when the spatial dimensions of our input images are larger than what the CNN expects – this is a common technique to help reduce overfitting, and is, therefore, **a form of regularization**. Instead of using the entire image during training, we instead crop a random portion of it and pass it to the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPeYByPoa6C4"
      },
      "source": [
        "As you will see, we will construct an HDF5 dataset of Kaggle Dogs vs. Cats images where each image is 256x256 pixels. However, the AlexNet architecture that we’ll be implementing later in this lesson can only accept images of size 227x227 pixels. This is an excellent opportunity to perform data augmentation by randomly cropping a 227x227 region from the 256x256 image during training using PatchPreprocessor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm2bRAs2UKHj"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.feature_extraction.image import extract_patches_2d\n",
        "\n",
        "class PatchPreprocessor:\n",
        "\tdef __init__(self, width, height):\n",
        "\t\t# store the target width and height of the image\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# extract a random crop from the image with the target width\n",
        "\t\t# and height\n",
        "\t\treturn extract_patches_2d(image, (self.height, self.width),\n",
        "\t\t\tmax_patches=1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gft-PE3aJvZ"
      },
      "source": [
        "### 3.4.4 Crop preprocessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFW6NqV2eSYY"
      },
      "source": [
        "Next, we need to define a CropPreprocessor responsible for computing the 10-crops for oversampling. During the evaluating phase of our CNN, we’ll crop the four corners of the input image + the center region and then take their corresponding horizontal flips, for a total of ten samples per input image.\n",
        "\n",
        "These ten samples will be passed through the CNN, and then the probabilities averaged.\n",
        "Applying this over-sampling method tends to include 1-2 percent increases in classification accuracy (and in some cases, even higher)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc0kftNdR2lk"
      },
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "class CropPreprocessor:\n",
        "\tdef __init__(self, width, height, horiz=True, inter=cv2.INTER_AREA):\n",
        "\t\t# store the target image width, height, whether or not\n",
        "\t\t# horizontal flips should be included, along with the\n",
        "\t\t# interpolation method used when resizing\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\t\tself.horiz = horiz\n",
        "\t\tself.inter = inter\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# initialize the list of crops\n",
        "\t\tcrops = []\n",
        "\n",
        "\t\t# grab the width and height of the image then use these\n",
        "\t\t# dimensions to define the corners of the image based\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\t\tcoords = [\n",
        "\t\t\t[0, 0, self.width, self.height],\n",
        "\t\t\t[w - self.width, 0, w, self.height],\n",
        "\t\t\t[w - self.width, h - self.height, w, h],\n",
        "\t\t\t[0, h - self.height, self.width, h]]\n",
        "\n",
        "\t\t# compute the center crop of the image as well\n",
        "\t\tdW = int(0.5 * (w - self.width))\n",
        "\t\tdH = int(0.5 * (h - self.height))\n",
        "\t\tcoords.append([dW, dH, w - dW, h - dH])\n",
        "\n",
        "\t\t# loop over the coordinates, extract each of the crops,\n",
        "\t\t# and resize each of them to a fixed size\n",
        "\t\tfor (startX, startY, endX, endY) in coords:\n",
        "\t\t\tcrop = image[startY:endY, startX:endX]\n",
        "\t\t\tcrop = cv2.resize(crop, (self.width, self.height),\n",
        "\t\t\t\tinterpolation=self.inter)\n",
        "\t\t\tcrops.append(crop)\n",
        "\n",
        "\t\t# check to see if the horizontal flips should be taken\n",
        "\t\tif self.horiz:\n",
        "\t\t\t# compute the horizontal mirror flips for each crop\n",
        "\t\t\tmirrors = [cv2.flip(c, 1) for c in crops]\n",
        "\t\t\tcrops.extend(mirrors)\n",
        "\n",
        "\t\t# return the set of crops\n",
        "\t\treturn np.array(crops)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NF8faSog8so"
      },
      "source": [
        "### 3.4.5 HDF5 dataset generators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soLUcyH1iCOO"
      },
      "source": [
        "Before we can implement the AlexNet architecture and train it on the Kaggle Dogs vs. Cats dataset, we first need to define a class responsible for yielding batches of images and labels from our HDF5 dataset. Section 3.3.3 discussed how to convert a set of images residing on disk into an HDF5 dataset – but how do we get them back out again? The answer is to define an **HDF5DatasetGenerator** class.\n",
        "\n",
        "Previously, all of our image datasets could be loaded into memory so we could rely on Keras generator utilities to yield our batches of images and corresponding labels. However, now that our datasets are too large to fit into memory, we need to handle implementing this generator ourselves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMo22QKUg7me"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "class HDF5DatasetGenerator:\n",
        "\tdef __init__(self, dbPath, batchSize, preprocessors=None, aug=None, binarize=True, classes=2):\n",
        "\t\t# store the batch size, preprocessors, and data augmentor,\n",
        "\t\t# whether or not the labels should be binarized, along with\n",
        "\t\t# the total number of classes\n",
        "\t\tself.batchSize = batchSize\n",
        "\t\tself.preprocessors = preprocessors\n",
        "\t\tself.aug = aug\n",
        "\t\tself.binarize = binarize\n",
        "\t\tself.classes = classes\n",
        "\n",
        "\t\t# open the HDF5 database for reading and determine the total\n",
        "\t\t# number of entries in the database\n",
        "\t\tself.db = h5py.File(dbPath, \"r\")\n",
        "\t\tself.numImages = self.db[\"labels\"].shape[0]\n",
        "\n",
        "\tdef generator(self, passes=np.inf):\n",
        "\t\t# initialize the epoch count\n",
        "\t\tepochs = 0\n",
        "\n",
        "\t\t# keep looping infinitely -- the model will stop once we have\n",
        "\t\t# reach the desired number of epochs\n",
        "\t\twhile epochs < passes:\n",
        "\t\t\t# loop over the HDF5 dataset\n",
        "\t\t\tfor i in np.arange(0, self.numImages, self.batchSize):\n",
        "\t\t\t\t# extract the images and labels from the HDF dataset\n",
        "\t\t\t\timages = self.db[\"images\"][i: i + self.batchSize]\n",
        "\t\t\t\tlabels = self.db[\"labels\"][i: i + self.batchSize]\n",
        "\n",
        "\t\t\t\t# check to see if the labels should be binarized\n",
        "\t\t\t\tif self.binarize:\n",
        "\t\t\t\t\tlabels = to_categorical(labels,\n",
        "\t\t\t\t\t\tself.classes)\n",
        "\n",
        "\t\t\t\t# check to see if our preprocessors are not None\n",
        "\t\t\t\tif self.preprocessors is not None:\n",
        "\t\t\t\t\t# initialize the list of processed images\n",
        "\t\t\t\t\tprocImages = []\n",
        "\n",
        "\t\t\t\t\t# loop over the images\n",
        "\t\t\t\t\tfor image in images:\n",
        "\t\t\t\t\t\t# loop over the preprocessors and apply each\n",
        "\t\t\t\t\t\t# to the image\n",
        "\t\t\t\t\t\tfor p in self.preprocessors:\n",
        "\t\t\t\t\t\t\timage = p.preprocess(image)\n",
        "\n",
        "\t\t\t\t\t\t# update the list of processed images\n",
        "\t\t\t\t\t\tprocImages.append(image)\n",
        "\n",
        "\t\t\t\t\t# update the images array to be the processed\n",
        "\t\t\t\t\t# images\n",
        "\t\t\t\t\timages = np.array(procImages)\n",
        "\n",
        "\t\t\t\t# if the data augmenator exists, apply it\n",
        "\t\t\t\tif self.aug is not None:\n",
        "\t\t\t\t\t(images, labels) = next(self.aug.flow(images,\n",
        "\t\t\t\t\t\tlabels, batch_size=self.batchSize))\n",
        "\n",
        "\t\t\t\t# yield a tuple of images and labels\n",
        "\t\t\t\tyield (images, labels)\n",
        "\n",
        "\t\t\t# increment the total number of epochs\n",
        "\t\t\tepochs += 1\n",
        "\n",
        "\tdef close(self):\n",
        "\t\t# close the database\n",
        "\t\tself.db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ou-_XnZqjrY"
      },
      "source": [
        "### 3.4.6 Simple preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Juf1YRHzqmeU"
      },
      "source": [
        "# import the necessary packages\n",
        "import cv2\n",
        "\n",
        "class SimplePreprocessor:\n",
        "\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n",
        "\t\t# store the target image width, height, and interpolation\n",
        "\t\t# method used when resizing\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\t\tself.inter = inter\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# resize the image to a fixed size, ignoring the aspect\n",
        "\t\t# ratio\n",
        "\t\treturn cv2.resize(image, (self.width, self.height),\n",
        "\t\t\tinterpolation=self.inter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bJUx9Prq2xE"
      },
      "source": [
        "### 3.4.7 Training monitor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1JGywYeq5Wj"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.callbacks import BaseLogger\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "class TrainingMonitor(BaseLogger):\n",
        "\tdef __init__(self, figPath, jsonPath=None, startAt=0):\n",
        "\t\t# store the output path for the figure, the path to the JSON\n",
        "\t\t# serialized file, and the starting epoch\n",
        "\t\tsuper(TrainingMonitor, self).__init__()\n",
        "\t\tself.figPath = figPath\n",
        "\t\tself.jsonPath = jsonPath\n",
        "\t\tself.startAt = startAt\n",
        "\n",
        "\tdef on_train_begin(self, logs={}):\n",
        "\t\t# initialize the history dictionary\n",
        "\t\tself.H = {}\n",
        "\n",
        "\t\t# if the JSON history path exists, load the training history\n",
        "\t\tif self.jsonPath is not None:\n",
        "\t\t\tif os.path.exists(self.jsonPath):\n",
        "\t\t\t\tself.H = json.loads(open(self.jsonPath).read())\n",
        "\n",
        "\t\t\t\t# check to see if a starting epoch was supplied\n",
        "\t\t\t\tif self.startAt > 0:\n",
        "\t\t\t\t\t# loop over the entries in the history log and\n",
        "\t\t\t\t\t# trim any entries that are past the starting\n",
        "\t\t\t\t\t# epoch\n",
        "\t\t\t\t\tfor k in self.H.keys():\n",
        "\t\t\t\t\t\tself.H[k] = self.H[k][:self.startAt]\n",
        "\n",
        "\tdef on_epoch_end(self, epoch, logs={}):\n",
        "\t\t# loop over the logs and update the loss, accuracy, etc.\n",
        "\t\t# for the entire training process\n",
        "\t\tfor (k, v) in logs.items():\n",
        "\t\t\tl = self.H.get(k, [])\n",
        "\t\t\tl.append(float(v))\n",
        "\t\t\tself.H[k] = l\n",
        "\n",
        "\t\t# check to see if the training history should be serialized\n",
        "\t\t# to file\n",
        "\t\tif self.jsonPath is not None:\n",
        "\t\t\tf = open(self.jsonPath, \"w\")\n",
        "\t\t\tf.write(json.dumps(self.H))\n",
        "\t\t\tf.close()\n",
        "\n",
        "\t\t# ensure at least two epochs have passed before plotting\n",
        "\t\t# (epoch starts at zero)\n",
        "\t\tif len(self.H[\"loss\"]) > 1:\n",
        "\t\t\t# plot the training loss and accuracy\n",
        "\t\t\tN = np.arange(0, len(self.H[\"loss\"]))\n",
        "\t\t\tplt.style.use(\"ggplot\")\n",
        "\t\t\tplt.figure()\n",
        "\t\t\tplt.plot(N, self.H[\"loss\"], label=\"train_loss\")\n",
        "\t\t\tplt.plot(N, self.H[\"val_loss\"], label=\"val_loss\")\n",
        "\t\t\tplt.plot(N, self.H[\"accuracy\"], label=\"train_acc\")\n",
        "\t\t\tplt.plot(N, self.H[\"val_accuracy\"], label=\"val_acc\")\n",
        "\t\t\tplt.title(\"Training Loss and Accuracy [Epoch {}]\".format(\n",
        "\t\t\t\tlen(self.H[\"loss\"])))\n",
        "\t\t\tplt.xlabel(\"Epoch #\")\n",
        "\t\t\tplt.ylabel(\"Loss/Accuracy\")\n",
        "\t\t\tplt.legend()\n",
        "\n",
        "\t\t\t# save the figure\n",
        "\t\t\tplt.savefig(self.figPath)\n",
        "\t\t\tplt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk2HBr6JAMQ8"
      },
      "source": [
        "### 3.4.8 Rank accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJCbMd1sAPP8"
      },
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "\n",
        "def rank5_accuracy(preds, labels):\n",
        "\t# initialize the rank-1 and rank-5 accuracies\n",
        "\trank1 = 0\n",
        "\trank5 = 0\n",
        "\n",
        "\t# loop over the predictions and ground-truth labels\n",
        "\tfor (p, gt) in zip(preds, labels):\n",
        "\t\t# sort the probabilities by their index in descending\n",
        "\t\t# order so that the more confident guesses are at the\n",
        "\t\t# front of the list\n",
        "\t\tp = np.argsort(p)[::-1]\n",
        "\n",
        "\t\t# check if the ground-truth label is in the top-5\n",
        "\t\t# predictions\n",
        "\t\tif gt in p[:5]:\n",
        "\t\t\trank5 += 1\n",
        "\n",
        "\t\t# check to see if the ground-truth is the #1 prediction\n",
        "\t\tif gt == p[0]:\n",
        "\t\t\trank1 += 1\n",
        "\n",
        "\t# compute the final rank-1 and rank-5 accuracies\n",
        "\trank1 /= float(len(preds))\n",
        "\trank5 /= float(len(preds))\n",
        "\n",
        "\t# return a tuple of the rank-1 and rank-5 accuracies\n",
        "\treturn (rank1, rank5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrmCQKbMHI-4"
      },
      "source": [
        "### 3.4.9 SimpleDatasetLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj4RbTs8HH62"
      },
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# helper to load images\n",
        "class SimpleDatasetLoader:\n",
        "\tdef __init__(self, preprocessors=None):\n",
        "\t\t# store the image preprocessor\n",
        "\t\tself.preprocessors = preprocessors\n",
        "\n",
        "\t\t# if the preprocessors are None, initialize them as an\n",
        "\t\t# empty list\n",
        "\t\tif self.preprocessors is None:\n",
        "\t\t\tself.preprocessors = []\n",
        "\n",
        "\tdef load(self, imagePaths, verbose=-1):\n",
        "\t\t# initialize the list of features and labels\n",
        "\t\tdata = []\n",
        "\t\tlabels = []\n",
        "\n",
        "\t\t# loop over the input images\n",
        "\t\tfor (i, imagePath) in enumerate(imagePaths):\n",
        "\t\t\t# load the image and extract the class label assuming\n",
        "\t\t\t# that our path has the following format:\n",
        "\t\t\t# /path/to/dataset/{class}/{image}.jpg\n",
        "\t\t\timage = cv2.imread(imagePath)\n",
        "\t\t\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "\t\t\t# check to see if our preprocessors are not None\n",
        "\t\t\tif self.preprocessors is not None:\n",
        "\t\t\t\t# loop over the preprocessors and apply each to\n",
        "\t\t\t\t# the image\n",
        "\t\t\t\tfor p in self.preprocessors:\n",
        "\t\t\t\t\timage = p.preprocess(image)\n",
        "\n",
        "\t\t\t# treat our processed image as a \"feature vector\"\n",
        "\t\t\t# by updating the data list followed by the labels\n",
        "\t\t\tdata.append(image)\n",
        "\t\t\tlabels.append(label)\n",
        "\n",
        "\t\t\t# show an update every `verbose` images\n",
        "\t\t\tif verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
        "\t\t\t\tprint(\"[INFO] processed {}/{}\".format(i + 1,\n",
        "\t\t\t\t\tlen(imagePaths)))\n",
        "\n",
        "\t\t# return a tuple of the data and labels\n",
        "\t\treturn (np.array(data), np.array(labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFt0U2oUmMuU"
      },
      "source": [
        "## 3.5 Implementing AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax0EAsPgmEea"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "class AlexNet:\n",
        "\t@staticmethod\n",
        "\tdef build(width, height, depth, classes, reg=0.0002):\n",
        "\t\t# initialize the model along with the input shape to be\n",
        "\t\t# \"channels last\" and the channels dimension itself\n",
        "\t\tmodel = Sequential()\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\t\tchanDim = -1\n",
        "\n",
        "\t\t# if we are using \"channels first\", update the input shape\n",
        "\t\t# and channels dimension\n",
        "\t\tif K.image_data_format() == \"channels_first\":\n",
        "\t\t\tinputShape = (depth, height, width)\n",
        "\t\t\tchanDim = 1\n",
        "\n",
        "\t\t# Block #1: first CONV => RELU => POOL layer set\n",
        "\t\tmodel.add(Conv2D(96, (11, 11), strides=(4, 4),input_shape=inputShape, padding=\"valid\",kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\t# Block #2: second CONV => RELU => POOL layer set\n",
        "\t\tmodel.add(Conv2D(256, (5, 5), padding=\"same\",kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\t# Block #3: CONV => RELU => CONV => RELU => CONV => RELU\n",
        "\t\tmodel.add(Conv2D(384, (3, 3), padding=\"same\",kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(384, (3, 3), padding=\"same\",kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(256, (3, 3), padding=\"same\",kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\t# Block #4: first set of FC => RELU layers\n",
        "\t\tmodel.add(Flatten())\n",
        "\t\tmodel.add(Dense(4096, kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization())\n",
        "\t\tmodel.add(Dropout(0.5))\n",
        "\n",
        "\t\t# Block #5: second set of FC => RELU layers\n",
        "\t\tmodel.add(Dense(4096, kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization())\n",
        "\t\tmodel.add(Dropout(0.5))\n",
        "\n",
        "\t\t# softmax classifier\n",
        "\t\tmodel.add(Dense(classes, kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"softmax\"))\n",
        "\n",
        "\t\t# return the constructed network architecture\n",
        "\t\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeWP4j7-nGuU"
      },
      "source": [
        "## 3.6 Training AlexNet on Kaggle: dogs vs cats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXM6nqiaqBh1"
      },
      "source": [
        "# define the path to the output training, validation, and testing\n",
        "# HDF5 files\n",
        "TRAIN_HDF5 = \"catdogs/hdf5/train.hdf5\"\n",
        "VAL_HDF5 = \"catdogs/hdf5/val.hdf5\"\n",
        "TEST_HDF5 = \"catdogs/hdf5/test.hdf5\"\n",
        "\n",
        "# path to the output model file\n",
        "MODEL_PATH = \"catdogs/alexnet_dogs_vs_cats.model\"\n",
        "\n",
        "# define the path to the dataset mean\n",
        "DATASET_MEAN = \"catdogs/dogs_vs_cats_mean.json\"\n",
        "\n",
        "# define the path to the output directory used for storing plots,\n",
        "# classification reports, etc.\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/Atividades/Ensino/Disciplinas/POS-GRADUAÇÃO/Deep Learning/Lessons/Lesson #06/catdogs/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmNyJTrMpfeY"
      },
      "source": [
        "# import the necessary packages\n",
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Configurations related to checkpoint\n",
        "# resume or not the model\n",
        "resume = True\n",
        "\n",
        "# checkpoint files\n",
        "filepath= OUTPUT_PATH + \"/epochs:{epoch:03d}-val_acc:{val_accuracy:.3f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "                         width_shift_range=0.2,\n",
        "                         height_shift_range=0.2,\n",
        "                         shear_range=0.15,\n",
        "                         horizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "# load the RGB means for the training set\n",
        "means = json.loads(open(DATASET_MEAN).read())\n",
        "\n",
        "# initialize the image preprocessors\n",
        "sp = SimplePreprocessor(227, 227)\n",
        "pp = PatchPreprocessor(227, 227)\n",
        "mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\n",
        "iap = ImageToArrayPreprocessor()\n",
        "\n",
        "# initialize the training and validation dataset generators\n",
        "trainGen = HDF5DatasetGenerator(TRAIN_HDF5, 128, aug=aug,preprocessors=[pp, mp, iap], classes=2)\n",
        "valGen = HDF5DatasetGenerator(VAL_HDF5, 128,preprocessors=[sp, mp, iap], classes=2)\n",
        "\n",
        "# initialize the optimizer\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(lr=1e-3)\n",
        "\n",
        "model = AlexNet.build(width=227, height=227, depth=3,classes=2, reg=0.0002)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
        "\n",
        "# construct the set of callbacks\n",
        "path = os.path.sep.join([OUTPUT_PATH, \"{}.png\".format(os.getpid())])\n",
        "callbacks = [TrainingMonitor(path)]\n",
        "\n",
        "initial_epoch = 1\n",
        "\n",
        "# load previou weights\n",
        "if resume == True:\n",
        "  model.load_weights(OUTPUT_PATH + \"/epochs:045-val_acc:0.921.hdf5\")\n",
        "  initial_epoch = 45\n",
        "\n",
        "# train the network\n",
        "history = model.fit(trainGen.generator(),\n",
        "          steps_per_epoch=trainGen.numImages // 128,\n",
        "          validation_data=valGen.generator(),\n",
        "          validation_steps=valGen.numImages // 128,\n",
        "          epochs=45,\n",
        "          max_queue_size=10,\n",
        "          callbacks=[callbacks,checkpoint], verbose=1,\n",
        "          initial_epoch=initial_epoch)\n",
        "\n",
        "# save the model to file\n",
        "print(\"[INFO] serializing model...\")\n",
        "model.save(MODEL_PATH, overwrite=True)\n",
        "\n",
        "# close the HDF5 datasets\n",
        "trainGen.close()\n",
        "valGen.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgP_SX7JBvSK"
      },
      "source": [
        "## 3.7 Evaluating AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8nruX7p86lq"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import progressbar\n",
        "import json\n",
        "\n",
        "# load the RGB means for the training set\n",
        "means = json.loads(open(DATASET_MEAN).read())\n",
        "\n",
        "# initialize the image preprocessors\n",
        "sp = SimplePreprocessor(227, 227)\n",
        "mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\n",
        "cp = CropPreprocessor(227, 227)\n",
        "iap = ImageToArrayPreprocessor()\n",
        "\n",
        "# load the pretrained network\n",
        "print(\"[INFO] loading model...\")\n",
        "model = load_model(MODEL_PATH)\n",
        "\n",
        "# initialize the testing dataset generator, then make predictions on\n",
        "# the testing data\n",
        "print(\"[INFO] predicting on test data (no crops)...\")\n",
        "testGen = HDF5DatasetGenerator(TEST_HDF5, 64,\n",
        "                               preprocessors=[sp, mp, iap], classes=2)\n",
        "predictions = model.predict(testGen.generator(),\n",
        "                            steps=testGen.numImages // 64, max_queue_size=10)\n",
        "\n",
        "# compute the rank-1 and rank-5 accuracies\n",
        "(rank1, _) = rank5_accuracy(predictions, testGen.db[\"labels\"])\n",
        "print(\"[INFO] rank-1: {:.2f}%\".format(rank1 * 100))\n",
        "testGen.close()\n",
        "\n",
        "\n",
        "# re-initialize the testing set generator, this time excluding the\n",
        "# `SimplePreprocessor`\n",
        "testGen = HDF5DatasetGenerator(TEST_HDF5, 64,\n",
        "                               preprocessors=[mp], classes=2)\n",
        "predictions = []\n",
        "\n",
        "# initialize the progress bar\n",
        "widgets = [\"Evaluating: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
        "pbar = progressbar.ProgressBar(maxval=testGen.numImages // 64,widgets=widgets).start()\n",
        "\n",
        "# loop over a single pass of the test data\n",
        "for (i, (images, labels)) in enumerate(testGen.generator(passes=1)):\n",
        "\t# loop over each of the individual images\n",
        "\tfor image in images:\n",
        "\t\t# apply the crop preprocessor to the image to generate 10\n",
        "\t\t# separate crops, then convert them from images to arrays\n",
        "\t\tcrops = cp.preprocess(image)\n",
        "\t\tcrops = np.array([iap.preprocess(c) for c in crops],\n",
        "\t\t\tdtype=\"float32\")\n",
        "\n",
        "\t\t# make predictions on the crops and then average them\n",
        "\t\t# together to obtain the final prediction\n",
        "\t\tpred = model.predict(crops)\n",
        "\t\tpredictions.append(pred.mean(axis=0))\n",
        "\n",
        "\t# update the progress bar\n",
        "\tpbar.update(i)\n",
        "\n",
        "# compute the rank-1 accuracy\n",
        "pbar.finish()\n",
        "print(\"[INFO] predicting on test data (with crops)...\")\n",
        "(rank1, _) = rank5_accuracy(predictions, testGen.db[\"labels\"])\n",
        "print(\"[INFO] rank-1: {:.2f}%\".format(rank1 * 100))\n",
        "testGen.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}